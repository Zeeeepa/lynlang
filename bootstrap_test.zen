// Bootstrap test for self-hosting compiler
// Tests that the Zen compiler components can compile themselves

io := @std.io
lexer := @std.compiler.lexer_enhanced
token := @std.compiler.token_enhanced

main = () i32 {
    io.print("=== Zen Bootstrap Test ===\n\n")
    
    // Test 1: Lexer can tokenize simple code
    io.print("Test 1: Lexer tokenization\n")
    io.print("--------------------------\n")
    
    code := "x := 42\ny := x + 10"
    io.print("Input code:\n")
    io.print(code)
    io.print("\n\n")
    
    // Create lexer
    lex := lexer.lexer_new(code)
    
    // Tokenize
    io.print("Tokens:\n")
    count := 0
    loop {
        tok := lexer.lexer_next_token(lex)
        
        tok.type == token.TokenType.EOF ?
            | true => { break }
            | false => {
                io.print("  ")
                io.print(token.token_type_to_string(tok.type))
                io.print(": '")
                io.print(tok.value)
                io.print("' @ line ")
                io.print_int(tok.line)
                io.print(", col ")
                io.print_int(tok.column)
                io.print("\n")
                count = count + 1
            }
    }
    
    io.print("\nTotal tokens: ")
    io.print_int(count)
    io.print("\n\n")
    
    // Test 2: Verify keywords are recognized
    io.print("Test 2: Keyword recognition\n")
    io.print("---------------------------\n")
    
    keywords := ["if", "else", "loop", "return", "struct", "enum", "fn"]
    i := 0
    loop (i < 7) {
        is_kw := token.is_keyword(keywords[i])
        io.print("  '")
        io.print(keywords[i])
        io.print("' is keyword: ")
        io.print(is_kw ? "true" : "false")
        io.print("\n")
        i = i + 1
    }
    
    io.print("\n=== Bootstrap test complete ===\n")
    return 0
}