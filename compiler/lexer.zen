// Self-hosted Zen Lexer
// This module tokenizes Zen source code

build = @std
core := build.import("core")
{ Vec, DynVec } = @std
string := build.import("string")
char := build.import("char")

// Token types
// Note: Using struct representation since enum syntax is not yet supported
TokenKind: {
    tag: string,    // Token type (e.g., "IntLiteral", "Plus", "Identifier")
    value: string,  // Optional value for literals/identifiers
}

// Token structure
Token: {
    kind: TokenKind,
    span: Span,
    line: u32,
    column: u32,
}

// Source span for error reporting
Span: {
    start: u32,
    end: u32,
}

// Lexer state
Lexer: {
    source: string,
    pos: u32,
    line: u32,
    column: u32,
    tokens: DynVec<Token>,
}

// Helper function to create simple tokens (operators, punctuation)
make_token = (tag: string, start: u32, end: u32, line: u32, column: u32) Token   {
    return Token {
        kind: TokenKind { tag: tag, value: "" },
        span: Span { start: start, end: end },
        line: line,
        column: column,
    }
}

// No keywords in Zen per language spec - everything is an identifier
// Built-in types are just identifiers that have special meaning in context
KEYWORDS := []

// Create a new lexer
lexer_new = (source: string) Lexer   {
    Lexer {
        source: source,
        pos: 0,
        line: 1,
        column: 1,
        tokens: DynVec.new(),
    }
}

// Check if character is whitespace
is_whitespace = (c: char) bool   {
    (c == ' ') ? { return true }
    (c == '\t') ? { return true }
    (c == '\r') ? { return true }
    (c == '\n') ? { return true }
    return false
}

// Check if character is a digit
is_digit = (c: char) bool   {
    c < '0' ? { return false }
    c > '9' ? { return false }
    return true
}

// Check if character is alphabetic
is_alpha = (c: char) bool   {
    (c >= 'a' && c <= 'z') ? { return true }
    (c >= 'A' && c <= 'Z') ? { return true }
    return false
}

// Check if character can start an identifier
is_ident_start = (c: char) bool   {
    is_alpha(c) ? { return true }
    c == '_' ? { return true }
    return false
}

// Check if character can continue an identifier
is_ident_continue = (c: char) bool   {
    is_ident_start(c) ? { return true }
    is_digit(c) ? { return true }
    return false
}

// Check if string is a keyword
is_keyword = (s: string) bool   {
    i := 0
    loop i < KEYWORDS.len() {
        KEYWORDS[i] == s ? 
         { return true }
        i = i + 1
    }
    return false
}

// Peek at current character without advancing
peek = (lexer: Ptr<Lexer>) char   {
    lexer.pos >= lexer.source.len() ? | true { return '\0' }
                                      | false { return lexer.source[lexer.pos] }
}

// Peek at next character
peek_next = (lexer: Ptr<Lexer>) char   {
    lexer.pos + 1 >= lexer.source.len() ? | true { return '\0' }
                                          | false { return lexer.source[lexer.pos + 1] }
}

// Advance position and return current character
advance = (lexer: Ptr<Lexer>) char   {
    lexer.pos >= lexer.source.len() ? | true { return '\0' }
                                      | false {
            c := lexer.source[lexer.pos]
            lexer.pos = lexer.pos + 1
            lexer.column = lexer.column + 1
            
            c == '\n' ?  {
                    lexer.line = lexer.line + 1
                    lexer.column = 1
                }
            
            return c
        }
}

// Skip whitespace
skip_whitespace = (lexer: Ptr<Lexer>) void   {
    loop is_whitespace(peek(lexer)) {
        advance(lexer)
    }
}

// Skip line comment
skip_line_comment = (lexer: Ptr<Lexer>) void   {
    // Skip //
    advance(lexer)
    advance(lexer)
    
    // Skip until newline
    loop peek(lexer) != '\n' && peek(lexer) != '\0' {
        advance(lexer)
    }
}

// Skip block comment
skip_block_comment = (lexer: Ptr<Lexer>) bool   {
    // Skip /*
    advance(lexer)
    advance(lexer)
    
    // Find */
    loop peek(lexer) != '\0' {
        peek(lexer) == '*' && peek_next(lexer) == '/' ?
            | true {
                advance(lexer)
                advance(lexer)
                return true
            }
            | false { advance(lexer) }
    }
    
    return false  // Unterminated block comment
}

// Lex a number literal
lex_number = (lexer: Ptr<Lexer>) Token   {
    start := lexer.pos
    start_line := lexer.line
    start_column := lexer.column
    
    // Check for hex literal
    peek(lexer) == '0' && (peek_next(lexer) == 'x' || peek_next(lexer) == 'X') ?
        | true {
            advance(lexer)  // 0
            advance(lexer)  // x
            
            // Read hex digits
            loop is_digit(peek(lexer)) || (peek(lexer) >= 'a' && peek(lexer) <= 'f') || 
                 (peek(lexer) >= 'A' && peek(lexer) <= 'F') {
                advance(lexer)
            }
        }
        | false {
            // Read decimal digits
            loop is_digit(peek(lexer)) {
                advance(lexer)
            }
            
            // Check for float
            peek(lexer) == '.' && is_digit(peek_next(lexer)) ? {
                    advance(lexer)  // .
                    
                    // Read fractional part
                    loop is_digit(peek(lexer)) {
                        advance(lexer)
                    }
                    
                    // Check for exponent
                    (peek(lexer) == 'e' || peek(lexer) == 'E') ? {
                            advance(lexer)
                            (peek(lexer) == '+' || peek(lexer) == '-') ? { advance(lexer) }
                            
                            loop is_digit(peek(lexer)) {
                                advance(lexer)
                            }
                        }
                    
                    // Return float token
                    value_str := string_slice(lexer.source, start, lexer.pos)
                    return Token {
                        kind: TokenKind { tag: "FloatLiteral", value: value_str },
                        span: Span { start: start, end: lexer.pos },
                        line: start_line,
                        column: start_column,
                    }
                }
        }
    
    // Return integer token
    value_str := string_slice(lexer.source, start, lexer.pos)
    return Token {
        kind: TokenKind { tag: "IntLiteral", value: value_str },
        span: Span { start: start, end: lexer.pos },
        line: start_line,
        column: start_column,
    }
}

// Lex a string literal
lex_string = (lexer: Ptr<Lexer>) Token   {
    start := lexer.pos
    start_line := lexer.line
    start_column := lexer.column
    
    quote := advance(lexer)  // " or '
    result := ""
    
    loop peek(lexer) != quote && peek(lexer) != '\0' {
        peek(lexer) == '\\' ?
            | true {
                advance(lexer)  // \
                c := advance(lexer)
                
                // Handle escape sequences
                c ?
                    | 'n' { result = result + "\n" }
                    | 't' { result = result + "\t" }
                    | 'r' { result = result + "\r" }
                    | '\\' { result = result + "\\" }
                    | '"' { result = result + "\"" }
                    | '\'' { result = result + "'" }
                    | _ { result = result + c }
            }
            | false { result = result + advance(lexer) }
    }
    
    peek(lexer) == quote ?
        | true {
            advance(lexer)
            
            // Character literal if single quote and single char
            quote == '\'' && result.len() == 1 ?
                | true { return Token {
                    kind: TokenKind { tag: "CharLiteral", value: result },
                    span: Span { start: start, end: lexer.pos },
                    line: start_line,
                    column: start_column,
                }}
                | false { return Token {
                    kind: TokenKind { tag: "StringLiteral", value: result },
                    span: Span { start: start, end: lexer.pos },
                    line: start_line,
                    column: start_column,
                }}
        }
        | false { return Token {
            kind: TokenKind { tag: "Invalid", value: "Unterminated string literal" },
            span: Span { start: start, end: lexer.pos },
            line: start_line,
            column: start_column,
        }}
}

// Lex an identifier or keyword
lex_identifier = (lexer: Ptr<Lexer>) Token   {
    start := lexer.pos
    start_line := lexer.line
    start_column := lexer.column
    
    // Read identifier characters
    loop is_ident_continue(peek(lexer)) {
        advance(lexer)
    }
    
    ident := string_slice(lexer.source, start, lexer.pos)
    
    // Check for boolean literals
    ident == "true" ? { return Token {
            kind: TokenKind { tag: "BoolLiteral", value: "true" },
            span: Span { start: start, end: lexer.pos },
            line: start_line,
            column: start_column,
        }}
    
    ident == "false" ? { return Token {
            kind: TokenKind { tag: "BoolLiteral", value: "false" },
            span: Span { start: start, end: lexer.pos },
            line: start_line,
            column: start_column,
        }}
    
    // Check if it's a keyword
    is_keyword(ident) ?
        | true { return Token {
            kind: TokenKind { tag: "Keyword", value: ident },
            span: Span { start: start, end: lexer.pos },
            line: start_line,
            column: start_column,
        }}
        | false { return Token {
            kind: TokenKind { tag: "Identifier", value: ident },
            span: Span { start: start, end: lexer.pos },
            line: start_line,
            column: start_column,
        }}
}

// Get next token
next_token = (lexer: Ptr<Lexer>) Token   {
    // Skip whitespace and comments
    loop {
        skip_whitespace(lexer)
        
        peek(lexer) == '/' && peek_next(lexer) == '/' ?
            | true { skip_line_comment(lexer) }
            | false {
                peek(lexer) == '/' && peek_next(lexer) == '*' ?
                    | true {
                        skip_block_comment(lexer) ?
                            | false { return Token {
                                kind: TokenKind { tag: "Invalid", value: "Unterminated block comment" },
                                span: Span { start: lexer.pos, end: lexer.pos },
                                line: lexer.line,
                                column: lexer.column,
                            }}
                            | true {}
                    }
                    | false { break }
            }
    }
    
    start_line := lexer.line
    start_column := lexer.column
    start := lexer.pos
    
    c := peek(lexer)
    
    // End of file
    c == '\0' ? { return Token {
            kind: TokenKind { tag: "Eof", value: "" },
            span: Span { start: start, end: start },
            line: start_line,
            column: start_column,
    }}
    
    // Numbers
    is_digit(c) ? { return lex_number(lexer) }

    
    // Strings and chars
    (c == '"' || c == '\'') ? { return lex_string(lexer) }
    
    // Identifiers and keywords
    is_ident_start(c) ? { return lex_identifier(lexer) }
    
    // Operators and punctuation
    c := advance(lexer)
    next := peek(lexer)
    
    // Two-character operators
    c ?
        | '+' {
            next == '=' ?
                | true {
                    advance(lexer)
                    return make_token("PlusAssign", start, lexer.pos, start_line, start_column)
                }
                | false { return make_token("Plus", start, lexer.pos, start_line, start_column) }
        }
        | '-' {
            next == '=' ?
                | true {
                    advance(lexer)
                    return make_token("MinusAssign", start, lexer.pos, start_line, start_column)
                }
                | false {
                    next == '>' ?
                        | true {
                            advance(lexer)
                            return make_token("Arrow", start, lexer.pos, start_line, start_column)
                        }
                        | false { return make_token("Minus", start, lexer.pos, start_line, start_column) }
                }
        }
        | '*' {
            next == '=' ?
                | true {
                    advance(lexer)
                    return make_token("StarAssign", start, lexer.pos, start_line, start_column)
                }
                | false { return make_token("Star", start, lexer.pos, start_line, start_column) }
        }
        | '/' {
            next == '=' ?
                | true {
                    advance(lexer)
                    return make_token("SlashAssign", start, lexer.pos, start_line, start_column)
                }
                | false { return make_token("Slash", start, lexer.pos, start_line, start_column) }
        }
        | '=' {
            next == '=' ?
                | true {
                    advance(lexer)
                    return make_token("Equal", start, lexer.pos, start_line, start_column)
                }
                | false {
                    next == '>' ?
                        | true {
                            advance(lexer)
                            return make_token("FatArrow", start, lexer.pos, start_line, start_column)
                        }
                        | false { return make_token("Assign", start, lexer.pos, start_line, start_column) }
                }
        }
        | '!' {
            next == '=' ?
                | true {
                    advance(lexer)
                    return make_token("NotEqual", start, lexer.pos, start_line, start_column)
                }
                | false { return Token {
                        kind: TokenKind { tag: "Invalid", value: "Unexpected character '!'" },
                        span: Span { start: start, end: lexer.pos },
                        line: start_line,
                        column: start_column,
                    }}
                }
        }
        | '<' {
            next == '=' ?
                | true {
                    advance(lexer)
                    return make_token("LessEqual", start, lexer.pos, start_line, start_column)
                }
                | false {
                    next == '<' ?
                        | true {
                            advance(lexer)
                            return make_token("LShift", start, lexer.pos, start_line, start_column)
                        }
                        | false { return make_token("Less", start, lexer.pos, start_line, start_column) }
                }
        }
        | '>' {
            next == '=' ?
                | true {
                    advance(lexer)
                    return make_token("GreaterEqual", start, lexer.pos, start_line, start_column)
                }
                | false {
                    next == '>' ?
                        | true {
                            advance(lexer)
                            return make_token("RShift", start, lexer.pos, start_line, start_column)
                        }
                        | false { return make_token("Greater", start, lexer.pos, start_line, start_column) }
                }
        }
        | ':' {
            next == ':' ?
                | true {
                    advance(lexer)
                    peek(lexer) == '=' ?
                        | true {
                            advance(lexer)
                            return make_token("ColonColonAssign", start, lexer.pos, start_line, start_column)
                        }
                        | false { return make_token("ColonColon", start, lexer.pos, start_line, start_column) }
                }
                | false {
                    next == '=' ?
                        | true {
                            advance(lexer)
                            return make_token("ColonAssign", start, lexer.pos, start_line, start_column)
                        }
                        | false { return make_token("Colon", start, lexer.pos, start_line, start_column) }
                }
        }
        | '.' {
            next == '.' ?
                | true {
                    advance(lexer)
                    return make_token("DotDot", start, lexer.pos, start_line, start_column)
                }
                | false { return make_token("Dot", start, lexer.pos, start_line, start_column) }
        }
        | '&' { return make_token("Ampersand", start, lexer.pos, start_line, start_column) }
        | '|' { return make_token("Pipe", start, lexer.pos, start_line, start_column) }
        | '^' { return make_token("Caret", start, lexer.pos, start_line, start_column) }
        | '~' { return make_token("Tilde", start, lexer.pos, start_line, start_column) }
        | '%' { return make_token("Percent", start, lexer.pos, start_line, start_column) }
        | '(' { return make_token("LeftParen", start, lexer.pos, start_line, start_column) }
        | ')' { return make_token("RightParen", start, lexer.pos, start_line, start_column) }
        | '{' { return make_token("LeftBrace", start, lexer.pos, start_line, start_column) }
        | '}' { return make_token("RightBrace", start, lexer.pos, start_line, start_column) }
        | '[' { return make_token("LeftBracket", start, lexer.pos, start_line, start_column) }
        | ']' { return make_token("RightBracket", start, lexer.pos, start_line, start_column) }
        | ';' { return make_token("Semicolon", start, lexer.pos, start_line, start_column) }
        | ',' { return make_token("Comma", start, lexer.pos, start_line, start_column) }
        | '?' { return make_token("Question", start, lexer.pos, start_line, start_column) }
        | '@' {
            // Check for @std or @this special symbols
            is_ident_start(peek(lexer)) ? {
                ident_start := lexer.pos
                loop is_ident_continue(peek(lexer)) {
                    advance(lexer)
                }
                ident := string_slice(lexer.source, ident_start, lexer.pos)
                (ident == "std" || ident == "this") ?
                    | true { return make_token("BuiltinSymbol", start, lexer.pos, start_line, start_column) }
                    | false { 
                        // Reset position and return just '@'
                        lexer.pos = ident_start
                        return make_token("At", start, ident_start, start_line, start_column)
                    }
            }
            return make_token("At", start, lexer.pos, start_line, start_column)
        }
        | '$' { return make_token("Dollar", start, lexer.pos, start_line, start_column) }
        | '#' { return make_token("Hash", start, lexer.pos, start_line, start_column) }
        | _ { return Token {
                kind: TokenKind { tag: "Invalid", value: "Unexpected character" },
                span: Span { start: start, end: lexer.pos },
                line: start_line,
                column: start_column,
            }}
        }
}


// Tokenize entire source
tokenize = (lexer: Ptr<Lexer>) DynVec<Token>   {
    loop {
        token := next_token(lexer)
        DynVec.push(&lexer.tokens, token)
        
        token.kind ?
            | TokenKind.Eof { break }
            | _ {}
    }
    
    return lexer.tokens
}

// Helper functions (stubs - would be in string module)
string_slice = (s: string, start: u32, end: u32) string   {
    // Implementation would extract substring
    return ""
}

string_to_i64 = (s: string) i64   {
    // Implementation would parse integer
    return 0
}

string_to_f64 = (s: string) f64   {
    // Implementation would parse float
    return 0.0
}