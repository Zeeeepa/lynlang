// Self-hosted Zen Lexer
// Tokenizes Zen source code

// Import standard modules
{ core } = @std.core
{ build } = @std.build

// Token types
TokenType:  
    // Literals
    IntegerLiteral,
    FloatLiteral,
    StringLiteral,
    CharLiteral,
    BoolLiteral,
    
    // Identifiers and keywords
    Identifier,
    Keyword,
    
    // Operators
    Plus,
    Minus,
    Star,
    Slash,
    Percent,
    Equal,
    NotEqual,
    Less,
    Greater,
    LessEqual,
    GreaterEqual,
    And,
    Or,
    Not,
    BitwiseAnd,
    BitwiseOr,
    BitwiseXor,
    BitwiseNot,
    LeftShift,
    RightShift,
    Assign,
    PlusAssign,
    MinusAssign,
    StarAssign,
    SlashAssign,
    Arrow,
    FatArrow,
    Dot,
    DotDot,
    Colon,
    DoubleColon,
    ColonEqual,
    DoubleColonEqual,
    Question,
    Pipe,
    
    // Delimiters
    LeftParen,
    RightParen,
    LeftBrace,
    RightBrace,
    LeftBracket,
    RightBracket,
    Comma,
    Semicolon,
    At,
    Hash,
    Dollar,
    Ampersand,
    Underscore,
    
    // Special
    Newline,
    Whitespace,
    Comment,
    Eof,
    Unknown,

// Token structure
Token: {
    type_: TokenType,
    value: RawPtr<i8>,
    line: i32,
    column: i32,
    length: i32,
}

// Lexer state
Lexer: {
    input: RawPtr<i8>,
    position: i32,
    read_position: i32,
    current_char: i8,
    line: i32,
    column: i32,
    input_length: i32,
}

// Create a new lexer
lexer_new = (input: RawPtr<i8>, length: i32) Lexer   {
    lexer := Lexer {
        input: input,
        position: 0,
        read_position: 0,
        current_char: 0,
        line: 1,
        column: 1,
        input_length: length,
    }
    
    // Read first character
    // Note: In real implementation, would pass by reference
    // For now, modifying directly
    
    return lexer
}

// Read next character
lexer_read_char = (lexer: *Lexer) void   {
    lexer.read_position >= lexer.input_length ?
        | true { { }
            lexer.current_char = 0  // EOF
        }
        | false { { }
            lexer.current_char = lexer.input[lexer.read_position]
            lexer.position = lexer.read_position
            lexer.read_position = lexer.read_position + 1
            
            // Update line and column
            lexer.current_char == '\n' ?
                | true { { }
                    lexer.line = lexer.line + 1
                    lexer.column = 1
                }
                | false { { }
                    lexer.column = lexer.column + 1
                }
        }
}

// Peek at next character without advancing
lexer_peek_char = (lexer: *Lexer) i8   {
    lexer.read_position >= lexer.input_length ?
        | true { return 0 }
        | false { return lexer.input[lexer.read_position] }
}

// Skip whitespace
lexer_skip_whitespace = (lexer: *Lexer) void   {
    loop {
        lexer.current_char == ' ' || lexer.current_char == '\t' || lexer.current_char == '\r' ?
            | true { { }
                lexer_read_char(lexer)
            }
            | false { { }
                break
            }
    }
}

// Skip line comment
lexer_skip_line_comment = (lexer: *Lexer) void   {
    loop {
        lexer.current_char == '\n' || lexer.current_char == 0 ?
            | true { break }
            | false { lexer_read_char(lexer) }
    }
}

// Skip block comment
lexer_skip_block_comment = (lexer: *Lexer) void   {
    loop {
        lexer.current_char == 0 ?
            | true { break }
            | false { { }
                lexer.current_char == '*' && lexer_peek_char(lexer) == '/' ?
                    | true { { }
                        lexer_read_char(lexer)  // Skip *
                        lexer_read_char(lexer)  // Skip /
                        break
                    }
                    | false { { }
                        lexer_read_char(lexer)
                    }
            }
    }
}

// Check if character is a digit
is_digit = (ch: i8) bool   {
    return ch >= '0' && ch <= '9'
}

// Check if character is alphabetic
is_alpha = (ch: i8) bool   {
    return (ch >= 'a' && ch <= 'z') || (ch >= 'A' && ch <= 'Z')
}

// Check if character is alphanumeric or underscore
is_alnum = (ch: i8) bool   {
    return is_alpha(ch) || is_digit(ch) || ch == '_'
}

// Read an identifier or keyword
lexer_read_identifier = (lexer: *Lexer) Token   {
    start_pos := lexer.position
    start_col := lexer.column
    
    // Read first character (already checked to be alpha or underscore)
    lexer_read_char(lexer)
    
    // Read remaining characters
    loop {
        is_alnum(lexer.current_char) ?
            | true { lexer_read_char(lexer) }
            | false { break }
    }
    
    length := lexer.position - start_pos
    
    // Check if it's a keyword
    // TODO: Implement keyword checking
    
    return Token {
        type_: TokenType:Identifier,
        value: lexer.input + start_pos,
        line: lexer.line,
        column: start_col,
        length: length,
    }
}

// Read a number (integer or float)
lexer_read_number = (lexer: *Lexer) Token   {
    start_pos := lexer.position
    start_col := lexer.column
    is_float := false
    
    // Read digits
    loop {
        is_digit(lexer.current_char) ?
            | true { lexer_read_char(lexer) }
            | false { { }
                lexer.current_char == '.' && is_digit(lexer_peek_char(lexer)) ?
                    | true { { }
                        is_float = true
                        lexer_read_char(lexer)  // Skip .
                        loop {
                            is_digit(lexer.current_char) ?
                                | true { lexer_read_char(lexer) }
                                | false { break }
                        }
                    }
                    | false { break }
            }
    }
    
    length := lexer.position - start_pos
    
    token_type := is_float ?
        | true { TokenType:FloatLiteral }
        | false { TokenType:IntegerLiteral }
    
    return Token {
        type_: token_type,
        value: lexer.input + start_pos,
        line: lexer.line,
        column: start_col,
        length: length,
    }
}

// Read a string literal
lexer_read_string = (lexer: *Lexer) Token   {
    start_pos := lexer.position
    start_col := lexer.column
    quote_char := lexer.current_char
    
    lexer_read_char(lexer)  // Skip opening quote
    
    loop {
        lexer.current_char == 0 ?
            | true { break  // EOF - unterminated string }
            | false { { }
                lexer.current_char == '\\' ?
                    | true { { }
                        lexer_read_char(lexer)  // Skip backslash
                        lexer.current_char != 0 ?
                            | true { lexer_read_char(lexer)  // Skip escaped char }
                            | false { break }
                    }
                    | false { { }
                        lexer.current_char == quote_char ?
                            | true { { }
                                lexer_read_char(lexer)  // Skip closing quote
                                break
                            }
                            | false { lexer_read_char(lexer) }
                    }
            }
    }
    
    length := lexer.position - start_pos
    
    return Token {
        type_: TokenType:StringLiteral,
        value: lexer.input + start_pos,
        line: lexer.line,
        column: start_col,
        length: length,
    }
}

// Get next token
lexer_next_token = (lexer: *Lexer) Token   {
    // Skip whitespace
    lexer_skip_whitespace(lexer)
    
    // Check for EOF
    lexer.current_char == 0 ?
        | true { { }
            return Token {
                type_: TokenType:Eof,
                value: 0,
                line: lexer.line,
                column: lexer.column,
                length: 0,
            }
        }
        | false { { }}
    
    start_col := lexer.column
    ch := lexer.current_char
    
    // Check for comments
    ch == '/' && lexer_peek_char(lexer) == '/' ?
        | true { { }
            lexer_skip_line_comment(lexer)
            return lexer_next_token(lexer)
        }
        | false { { }}
    
    ch == '/' && lexer_peek_char(lexer) == '*' ?
        | true { { }
            lexer_skip_block_comment(lexer)
            return lexer_next_token(lexer)
        }
        | false { { }}
    
    // Check for identifiers and keywords
    is_alpha(ch) || ch == '_' ?
        | true { return lexer_read_identifier(lexer) }
        | false { { }}
    
    // Check for numbers
    is_digit(ch) ?
        | true { return lexer_read_number(lexer) }
        | false { { }}
    
    // Check for strings
    ch == '"' || ch == '\'' ?
        | true { return lexer_read_string(lexer) }
        | false { { }}
    
    // Check for operators and symbols
    token := Token {
        type_: TokenType:Unknown,
        value: lexer.input + lexer.position,
        line: lexer.line,
        column: start_col,
        length: 1,
    }
    
    ch ?
        | '+' { { }
            lexer_read_char(lexer)
            lexer.current_char == '=' ?
                | true { { }
                    token.type_ = TokenType:PlusAssign
                    token.length = 2
                    lexer_read_char(lexer)
                }
                | false { token.type_ = TokenType:Plus }
        }
        | '-' { { }
            lexer_read_char(lexer)
            lexer.current_char == '>' ?
                | true { { }
                    token.type_ = TokenType:Arrow
                    token.length = 2
                    lexer_read_char(lexer)
                }
                | false { { }
                    lexer.current_char == '=' ?
                        | true { { }
                            token.type_ = TokenType:MinusAssign
                            token.length = 2
                            lexer_read_char(lexer)
                        }
                        | false { token.type_ = TokenType:Minus }
                }
        }
        | '*' { { }
            lexer_read_char(lexer)
            lexer.current_char == '=' ?
                | true { { }
                    token.type_ = TokenType:StarAssign
                    token.length = 2
                    lexer_read_char(lexer)
                }
                | false { token.type_ = TokenType:Star }
        }
        | '/' { { }
            lexer_read_char(lexer)
            lexer.current_char == '=' ?
                | true { { }
                    token.type_ = TokenType:SlashAssign
                    token.length = 2
                    lexer_read_char(lexer)
                }
                | false { token.type_ = TokenType:Slash }
        }
        | '=' => {
            lexer_read_char(lexer)
            lexer.current_char == '=' ?
                | true { { }
                    token.type_ = TokenType:Equal
                    token.length = 2
                    lexer_read_char(lexer)
                }
                | false { { }
                    lexer.current_char == '>' ?
                        | true { { }
                            token.type_ = TokenType:FatArrow
                            token.length = 2
                            lexer_read_char(lexer)
                        }
                        | false { token.type_ = TokenType:Assign }
                }
        }
        | '<' { { }
            lexer_read_char(lexer)
            lexer.current_char == '=' ?
                | true { { }
                    token.type_ = TokenType:LessEqual
                    token.length = 2
                    lexer_read_char(lexer)
                }
                | false { { }
                    lexer.current_char == '<' ?
                        | true { { }
                            token.type_ = TokenType:LeftShift
                            token.length = 2
                            lexer_read_char(lexer)
                        }
                        | false { token.type_ = TokenType:Less }
                }
        }
        | '>' { { }
            lexer_read_char(lexer)
            lexer.current_char == '=' ?
                | true { { }
                    token.type_ = TokenType:GreaterEqual
                    token.length = 2
                    lexer_read_char(lexer)
                }
                | false { { }
                    lexer.current_char == '>' ?
                        | true { { }
                            token.type_ = TokenType:RightShift
                            token.length = 2
                            lexer_read_char(lexer)
                        }
                        | false { token.type_ = TokenType:Greater }
                }
        }
        | '!' { { }
            lexer_read_char(lexer)
            lexer.current_char == '=' ?
                | true { { }
                    token.type_ = TokenType:NotEqual
                    token.length = 2
                    lexer_read_char(lexer)
                }
                | false { token.type_ = TokenType:Not }
        }
        | '&' { { }
            lexer_read_char(lexer)
            lexer.current_char == '&' ?
                | true { { }
                    token.type_ = TokenType:And
                    token.length = 2
                    lexer_read_char(lexer)
                }
                | false { token.type_ = TokenType:Ampersand }
        }
        | '|' { { }
            lexer_read_char(lexer)
            lexer.current_char == '|' ?
                | true { { }
                    token.type_ = TokenType:Or
                    token.length = 2
                    lexer_read_char(lexer)
                }
                | false { token.type_ = TokenType:Pipe }
        }
        | ':' { { }
            lexer_read_char(lexer)
            lexer.current_char == ':' ?
                | true { { }
                    lexer_read_char(lexer)
                    lexer.current_char == '=' ?
                        | true { { }
                            token.type_ = TokenType:DoubleColonEqual
                            token.length = 3
                            lexer_read_char(lexer)
                        }
                        | false { { }
                            token.type_ = TokenType:DoubleColon
                            token.length = 2
                        }
                }
                | false { { }
                    lexer.current_char == '=' ?
                        | true { { }
                            token.type_ = TokenType:ColonEqual
                            token.length = 2
                            lexer_read_char(lexer)
                        }
                        | false { token.type_ = TokenType:Colon }
                }
        }
        | '.' { { }
            lexer_read_char(lexer)
            lexer.current_char == '.' ?
                | true { { }
                    token.type_ = TokenType:DotDot
                    token.length = 2
                    lexer_read_char(lexer)
                }
                | false { token.type_ = TokenType:Dot }
        }
        | '(' { { }
            token.type_ = TokenType:LeftParen
            lexer_read_char(lexer)
        }
        | ')' { { }
            token.type_ = TokenType:RightParen
            lexer_read_char(lexer)
        }
        | '{' { { }
            token.type_ = TokenType:LeftBrace
            lexer_read_char(lexer)
        }
        | '}' { { }
            token.type_ = TokenType:RightBrace
            lexer_read_char(lexer)
        }
        | '[' { { }
            token.type_ = TokenType:LeftBracket
            lexer_read_char(lexer)
        }
        | ']' { { }
            token.type_ = TokenType:RightBracket
            lexer_read_char(lexer)
        }
        | ',' { { }
            token.type_ = TokenType:Comma
            lexer_read_char(lexer)
        }
        | ';' { { }
            token.type_ = TokenType:Semicolon
            lexer_read_char(lexer)
        }
        | '?' { { }
            token.type_ = TokenType:Question
            lexer_read_char(lexer)
        }
        | '@' { { }
            token.type_ = TokenType:At
            lexer_read_char(lexer)
        }
        | '#' { { }
            token.type_ = TokenType:Hash
            lexer_read_char(lexer)
        }
        | '$' { { }
            token.type_ = TokenType:Dollar
            lexer_read_char(lexer)
        }
        | '%' { { }
            token.type_ = TokenType:Percent
            lexer_read_char(lexer)
        }
        | '^' { { }
            token.type_ = TokenType:BitwiseXor
            lexer_read_char(lexer)
        }
        | '~' { { }
            token.type_ = TokenType:BitwiseNot
            lexer_read_char(lexer)
        }
        | '\n' { { }
            token.type_ = TokenType:Newline
            lexer_read_char(lexer)
        }
        | _ { { }
            lexer_read_char(lexer)
        }
    
    return token
}

// Export public functions and types
export {
    TokenType,
    Token,
    Lexer,
    lexer_new,
    lexer_next_token,
    is_digit,
    is_alpha,
    is_alnum,
}