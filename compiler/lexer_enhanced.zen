// Enhanced Zen Lexer for Self-Hosting
// Complete tokenization for all Zen language features

{ io } = @std.io
{ string } = @std.string
{ vec } = @std.vec
{ core } = @std.core

// Token types covering all Zen syntax
TokenType: 
    // Literals
    Identifier:  { value: string},
    Number:  { value: string},
    String:  { value: string},
    Char:  { value: char},
    
    // Keywords
    Return, Loop, Break, Continue,
    Struct, Enum, Trait, Impl,
    Comptime, Type, Const, Let,
    True, False, Null,
    
    // Operators
    Plus, Minus, Star, Slash, Percent,
    Equal, NotEqual, Less, Greater, LessEqual, GreaterEqual,
    And, Or, Not,
    Assign, ColonEqual, ColonColonEqual,
    Arrow, FatArrow,
    Question, Pipe, At, Dollar,
    
    // Symbols
    LeftParen, RightParen,
    LeftBrace, RightBrace,
    LeftBracket, RightBracket,
    Comma, Dot, Colon, DoubleColon, Semicolon,
    
    // Special
    Newline, Eof, Invalid,

// Token with position information
Token: {
    type: TokenType,
    lexeme: string,
    line: u32,
    column: u32,
}

// Lexer state
Lexer: {
    input: string,
    tokens: vec.Vec<Token>,
    current: u32,
    start: u32,
    line: u32,
    column: u32,
}

// Create a new lexer
lexer_new = (input: string) Lexer {
    return Lexer{
        input: input,
        tokens: vec.new<Token>(),
        current: 0,
        start: 0,
        line: 1,
        column: 1,
    }
}

// Scan all tokens
lexer_scan_tokens = (l: MutPtr<Lexer>) vec.Vec<Token> {
    loop(() {
        is_at_end(l) ? 
            | true { break } 
            | false { }
        l.val.start = l.val.current
        scan_token(l)
    })
    
    // Add EOF token
    l.val.tokens.push(Token{
        type: TokenType.Eof,
        lexeme: "",
        line: l.val.line,
        column: l.val.column,
    })
    
    return l.val.tokens
}

// Check if at end of input
is_at_end = (l: MutPtr<Lexer>) bool {
    return l.val.current >= l.val.input.len()
}

// Get current character
peek = (l: MutPtr<Lexer>) char {
    is_at_end(l) ? 
        | true { return '\0' } 
        | false { return l.val.input[l.val.current] }
}

// Get next character without advancing
peek_next = (l: MutPtr<Lexer>) char {
    (l.val.current + 1) >= l.val.input.len() ? 
        | true { return '\0' } 
        | false { return l.val.input[l.val.current + 1] }
}

// Advance and return current character
advance = (l: MutPtr<Lexer>) char {
    ch := l.val.input[l.val.current]
    l.val.current = l.val.current + 1
    l.val.column = l.val.column + 1
    return ch
}

// Check if current matches expected and advance if so
match_char = (l: MutPtr<Lexer>, expected: char) bool {
    is_at_end(l) ? 
        | true { return false } 
        | false { }
    l.val.input[l.val.current] != expected ? 
        | true { return false } 
        | false { }
    l.val.current = l.val.current + 1
    l.val.column = l.val.column + 1
    return true
}

// Add a token
add_token = (l: MutPtr<Lexer>, type: TokenType) void {
    text := string.substring(l.val.input, l.val.start, l.val.current)
    l.val.tokens.push(Token{
        type: type,
        lexeme: text,
        line: l.val.line,
        column: l.val.column - (l.val.current - l.val.start),
    })
}

// Scan a single token
scan_token = (l: MutPtr<Lexer>) void {
    c := advance(l)
    
    c ?
        // Single character tokens
        | '(' { add_token(l, TokenType.LeftParen) }
        | ')' { add_token(l, TokenType.RightParen) }
        | '{' { add_token(l, TokenType.LeftBrace) }
        | '}' { add_token(l, TokenType.RightBrace) }
        | '[' { add_token(l, TokenType.LeftBracket) }
        | ']' { add_token(l, TokenType.RightBracket) }
        | ',' { add_token(l, TokenType.Comma) }
        | '.' { add_token(l, TokenType.Dot) }
        | ';' { add_token(l, TokenType.Semicolon) }
        | '+' { add_token(l, TokenType.Plus) }
        | '*' { add_token(l, TokenType.Star) }
        | '%' { add_token(l, TokenType.Percent) }
        | '?' { add_token(l, TokenType.Question) }
        | '|' { add_token(l, TokenType.Pipe) }
        | '@' { add_token(l, TokenType.At) }
        | '$' { add_token(l, TokenType.Dollar) }
        
        // Multi-character tokens
        | '-' {
            match_char(l, '>') ? 
                | true { add_token(l, TokenType.Arrow) } 
                | false { add_token(l, TokenType.Minus) }
        }
        | '=' {
            match_char(l, '=') ? 
                | true { add_token(l, TokenType.Equal) }
                | false { 
                    match_char(l, '>') ?
                        | true { add_token(l, TokenType.FatArrow) }
                        | false { add_token(l, TokenType.Assign) }
                }
        }
        | '!' {
            match_char(l, '=') ? 
                | true { add_token(l, TokenType.NotEqual) } 
                | false { add_token(l, TokenType.Not) }
        }
        | '<' {
            match_char(l, '=') ? 
                | true { add_token(l, TokenType.LessEqual) } 
                | false { add_token(l, TokenType.Less) }
        }
        | '>' {
            match_char(l, '=') ? 
                | true { add_token(l, TokenType.GreaterEqual) } 
                | false { add_token(l, TokenType.Greater) }
        }
        | ':' {
            match_char(l, ':') ? 
                | true {
                    match_char(l, '=') ?
                        | true { add_token(l, TokenType.ColonColonEqual) }
                        | false { add_token(l, TokenType.DoubleColon) }
                }
                | false {
                    match_char(l, '=') ?
                        | true { add_token(l, TokenType.ColonEqual) }
                        | false { add_token(l, TokenType.Colon) }
                }
        }
        | '&' {
            match_char(l, '&') ? 
                | true { add_token(l, TokenType.And) } 
                | false { add_token(l, TokenType.Invalid) }
        }
        | '/' {
            match_char(l, '/') ? 
                | true {
                    // Single-line comment - skip to end of line
                    loop(() {
                        (peek(l) != '\n' && !is_at_end(l)) ?
                            | true { advance(l) }
                            | false { break }
                    })
                }
                | false {
                    match_char(l, '*') ? 
                        | true {
                            // Block comment - skip until */
                            loop(() {
                                is_at_end(l) ?
                                    | true { break }
                                    | false { }
                                (peek(l) == '*' && peek_next(l) == '/') ?
                                    | true {
                                        advance(l)  // consume *
                                        advance(l)  // consume /
                                        break
                                    }
                                    | false { }
                                peek(l) == '\n' ?
                                    | true {
                                        l.val.line = l.val.line + 1
                                        l.val.column = 1
                                    }
                                    | false { }
                                advance(l)
                            })
                        }
                        | false { add_token(l, TokenType.Slash) }
                }
        }
        
        // Whitespace
        | ' ' | '\r' | '\t' {} // Ignore
        | '\n' {
            l.val.line = l.val.line + 1
            l.val.column = 1
        }
        
        // String literals
        | '"' { scan_string(l) }
        
        // Character literals
        | '\'' { scan_char(l) }
        
        // Default cases
        | _ {
            is_digit(c) ? 
                | true { scan_number(l) }
                | false {
                    is_alpha(c) ? 
                        | true { scan_identifier(l) }
                        | false { add_token(l, TokenType.Invalid) }
                }
        }
    }
}

// Scan string literal
scan_string = (l: MutPtr<Lexer>) void {
    loop(() {
        (peek(l) != '"' && !is_at_end(l)) ?
            | true {
                peek(l) == '\n' ? 
                    | true {
                        l.val.line = l.val.line + 1
                        l.val.column = 1
                    }
                    | false { }
                advance(l)
            }
            | false { break }
    })
    
    is_at_end(l) ? 
        | true {
            add_token(l, TokenType.Invalid)
            return
        }
        | false { }
    
    // Consume closing "
    advance(l)
    
    // Extract string value (without quotes)
    value := string.substring(l.val.input, l.val.start + 1, l.val.current - 1)
    add_token(l, TokenType.String { value: value })
}

// Scan character literal
scan_char = (l: MutPtr<Lexer>) void {
    peek(l) == '\\' ? 
        | true {
            advance(l) // Skip backslash
            advance(l) // Skip escape character
        }
        | false {
            advance(l) // Regular character
        }
    
    peek(l) != '\'' ? 
        | true {
            add_token(l, TokenType.Invalid)
            return
        }
        | false { }
    
    advance(l) // Consume closing '
    
    // Extract character value
    value := l.val.input[l.val.start + 1]
    add_token(l, TokenType.Char { value: value })
}

// Scan number literal
scan_number = (l: MutPtr<Lexer>) void {
    loop(() {
        is_digit(peek(l)) ?
            | true { advance(l) }
            | false { break }
    })
    
    // Look for decimal part
    (peek(l) == '.' && is_digit(peek_next(l))) ? 
        | true {
            advance(l) // Consume '.'
            loop(() {
                is_digit(peek(l)) ?
                    | true { advance(l) }
                    | false { break }
            })
        }
        | false { }
    
    value := string.substring(l.val.input, l.val.start, l.val.current)
    add_token(l, TokenType.Number { value: value })
}

// Scan identifier or keyword
scan_identifier = (l: MutPtr<Lexer>) void {
    loop(() {
        is_alphanumeric(peek(l)) ?
            | true { advance(l) }
            | false { break }
    })
    
    text := string.substring(l.val.input, l.val.start, l.val.current)
    type := get_keyword_type(text)
    add_token(l, type)
}

// Check if character is digit
is_digit = (c: char) bool   {
    return c >= '0' && c <= '9'
}

// Check if character is alpha
is_alpha = (c: char) bool   {
    return (c >= 'a' && c <= 'z') || 
           (c >= 'A' && c <= 'Z') || 
           c == '_'
}

// Check if character is alphanumeric
is_alphanumeric = (c: char) bool   {
    return is_alpha(c) || is_digit(c)
}

// Get keyword token type or identifier
get_keyword_type = (text: string) TokenType {
    text ?
        | "return" { TokenType.Return }
        | "loop" { TokenType.Loop }
        | "break" { TokenType.Break }
        | "continue" { TokenType.Continue }
        | "struct" { TokenType.Struct }
        | "enum" { TokenType.Enum }
        | "trait" { TokenType.Trait }
        | "impl" { TokenType.Impl }
        | "comptime" { TokenType.Comptime }
        | "type" { TokenType.Type }
        | "const" { TokenType.Const }
        | "let" { TokenType.Let }
        | "true" { TokenType.True }
        | "false" { TokenType.False }
        | "null" { TokenType.Null }
        | _ { TokenType.Identifier { value: text } }
}