// Simple self-hosted lexer for Zen
// A minimal lexer implementation for bootstrapping

core  = @std.core
io  = @std.io

// Token types
TokenType: 
    // Literals
    IntLiteral
    FloatLiteral
    StringLiteral
    BoolLiteral
    
    // Identifiers and keywords
    Identifier
    Keyword
    
    // Operators
    Plus
    Minus
    Star
    Slash
    Percent
    Equal
    NotEqual
    Less
    Greater
    LessEqual
    GreaterEqual
    And
    Or
    Not
    
    // Assignment
    Assign       // =
    ColonAssign  // :=
    
    // Delimiters
    LeftParen
    RightParen
    LeftBrace
    RightBrace
    LeftBracket
    RightBracket
    
    // Punctuation
    Comma
    Semicolon
    Colon
    Dot
    Arrow        // ->
    DoubleArrow  // =>
    
    // Special
    At           // @
    Hash         // #
    Question     // ?
    Pipe         // |
    
    // End of file
    Eof
    
    // Error
    Error

// Token structure
Token: {
    type: TokenType
    value: string
    line: i32
    column: i32
}

// Lexer state
Lexer: {
    input: string
    position: i32
    read_position: i32
    current_char: i8
    line: i32
    column: i32
}

// Create a new lexer
new_lexer = (input: string) Lexer {
    lex := Lexer {
        input: input
        position: 0
        read_position: 0
        current_char: 0
        line: 1
        column: 1
    }
    
    // Read the first character
    read_char(lex.mut_ref())
    return lex
}

// Read the next character
read_char = (lex: MutPtr<Lexer>) void {
    (lex.val.read_position >= string_len(lex.val.input)) ?
        | true {
            lex.val.current_char = 0  // EOF
        }
        | false {
            lex.val.current_char = string_char_at(lex.val.input, lex.val.read_position)
        }
    
    lex.val.position = lex.val.read_position
    lex.val.read_position = lex.val.read_position + 1
    
    // Update line and column
    (lex.val.current_char == '\n') ?
        | true {
            lex.val.line = lex.val.line + 1
            lex.val.column = 1
        }
        | false {
            lex.val.column = lex.val.column + 1
        }
}

// Peek at the next character without advancing
peek_char = (lex: MutPtr<Lexer>) i8 {
    (lex.val.read_position >= string_len(lex.val.input)) ?
        | true {
            return 0
        }
        | false {
            return string_char_at(lex.val.input, lex.val.read_position)
        }
}

// Skip whitespace
skip_whitespace = (lex: MutPtr<Lexer>) void {
    loop(() {
        (lex.val.current_char == ' ' || lex.val.current_char == '\t' || 
         lex.val.current_char == '\n' || lex.val.current_char == '\r') ?
            | true {
                read_char(lex)
            }
            | false {
                break
            }
    })
}

// Skip line comments
skip_line_comment = (lex: MutPtr<Lexer>) void {
    // Skip until end of line
    loop(() {
        (lex.val.current_char != '\n' && lex.val.current_char != 0) ?
            | true {
                read_char(lex)
            }
            | false {
                break
            }
    })
}

// Read an identifier or keyword
read_identifier = (lex: MutPtr<Lexer>) string {
    start_pos := lex.val.position
    
    // Read while alphanumeric or underscore
    loop(() {
        (is_letter(lex.val.current_char) || is_digit(lex.val.current_char) || 
         lex.val.current_char == '_') ?
            | true {
                read_char(lex)
            }
            | false {
                break
            }
    })
    
    return string_substring(lex.val.input, start_pos, lex.val.position)
}

// Read a number
read_number = (lex: MutPtr<Lexer>) string {
    start_pos := lex.val.position
    has_dot := false
    
    loop(() {
        (is_digit(lex.val.current_char) || lex.val.current_char == '.') ?
            | true {
                (lex.val.current_char == '.') ?
                    | true {
                        has_dot ?
                            | true {
                                break  // Second dot, stop reading
                            }
                            | false {
                                has_dot = true
                            }
                    }
                    | false { }
                read_char(lex)
            }
            | false {
                break
            }
    })
    
    return string_substring(lex.val.input, start_pos, lex.val.position)
}

// Read a string literal
read_string = (lex: MutPtr<Lexer>) string {
    read_char(lex)  // Skip opening quote
    start_pos := lex.val.position
    
    loop(() {
        (lex.val.current_char != '"' && lex.val.current_char != 0) ?
            | true {
                (lex.val.current_char == '\\') ?
                    | true {
                        read_char(lex)  // Skip escape character
                        (lex.val.current_char != 0) ?
                            | true {
                                read_char(lex)  // Skip escaped character
                            }
                            | false { }
                    }
                    | false {
                        read_char(lex)
                    }
            }
            | false {
                break
            }
    })
    
    result := string_substring(lex.val.input, start_pos, lex.val.position)
    
    (lex.val.current_char == '"') ?
        | true {
            read_char(lex)  // Skip closing quote
        }
        | false { }
    
    return result
}

// Check if character is a letter
is_letter = (ch: i8) bool {
    return (ch >= 'a' && ch <= 'z') || (ch >= 'A' && ch <= 'Z')
}

// Check if character is a digit
is_digit = (ch: i8) bool {
    return ch >= '0' && ch <= '9'
}

// Check if identifier is a keyword
is_keyword = (ident: string) bool {
    // Check common keywords
    (ident == "if" || ident == "else" || ident == "while" || ident == "for") ?
        | true {
            return true
        }
        | false { }
    (ident == "return" || ident == "break" || ident == "continue") ?
        | true {
            return true
        }
        | false { }
    if ident == "struct" || ident == "enum" || ident == "fn" {
        return true
    }
    if ident == "let" || ident == "mut" || ident == "const" {
        return true
    }
    if ident == "true" || ident == "false" {
        return true
    }
    if ident == "comptime" || ident == "extern" || ident == "match" {
        return true
    }
    return false
}

// Get the next token
next_token = (lex: *Lexer) Token   {
    skip_whitespace(lex)
    
    // Skip comments
    while lex.current_char == '/' && peek_char(lex) == '/' {
        skip_line_comment(lex)
        skip_whitespace(lex)
    }
    
    tok := Token {
        type: TokenType.Error
        value: ""
        line: lex.line
        column: lex.column
    }
    
    // Check for EOF
    if lex.current_char == 0 {
        tok.type = TokenType.Eof
        return tok
    }
    
    // Check for identifiers and keywords
    if is_letter(lex.current_char) || lex.current_char == '_' {
        ident := read_identifier(lex)
        
        if is_keyword(ident) {
            tok.type = TokenType.Keyword
        } else if ident == "true" || ident == "false" {
            tok.type = TokenType.BoolLiteral
        } else {
            tok.type = TokenType.Identifier
        }
        tok.value = ident
        return tok
    }
    
    // Check for numbers
    if is_digit(lex.current_char) {
        num := read_number(lex)
        
        if string_contains(num, ".") {
            tok.type = TokenType.FloatLiteral
        } else {
            tok.type = TokenType.IntLiteral
        }
        tok.value = num
        return tok
    }
    
    // Check for strings
    if lex.current_char == '"' {
        tok.type = TokenType.StringLiteral
        tok.value = read_string(lex)
        return tok
    }
    
    // Check for operators and punctuation
    ch := lex.current_char
    next_ch := peek_char(lex)
    
    // Two-character operators
    if ch == ':' && next_ch == '=' {
        tok.type = TokenType.ColonAssign
        tok.value = ":="
        read_char(lex)
        read_char(lex)
        return tok
    }
    
    if ch == '-' && next_ch == '>' {
        tok.type = TokenType.Arrow
        tok.value = "->"
        read_char(lex)
        read_char(lex)
        return tok
    }
    
    if ch == '=' && next_ch == '>' {
        tok.type = TokenType.DoubleArrow
        tok.value = "=>"
        read_char(lex)
        read_char(lex)
        return tok
    }
    
    if ch == '=' && next_ch == '=' {
        tok.type = TokenType.Equal
        tok.value = "=="
        read_char(lex)
        read_char(lex)
        return tok
    }
    
    if ch == '!' && next_ch == '=' {
        tok.type = TokenType.NotEqual
        tok.value = "!="
        read_char(lex)
        read_char(lex)
        return tok
    }
    
    if ch == '<' && next_ch == '=' {
        tok.type = TokenType.LessEqual
        tok.value = "<="
        read_char(lex)
        read_char(lex)
        return tok
    }
    
    if ch == '>' && next_ch == '=' {
        tok.type = TokenType.GreaterEqual
        tok.value = ">="
        read_char(lex)
        read_char(lex)
        return tok
    }
    
    if ch == '&' && next_ch == '&' {
        tok.type = TokenType.And
        tok.value = "&&"
        read_char(lex)
        read_char(lex)
        return tok
    }
    
    if ch == '|' && next_ch == '|' {
        tok.type = TokenType.Or
        tok.value = "||"
        read_char(lex)
        read_char(lex)
        return tok
    }
    
    // Single-character tokens
    ch ?{
        '+' => {
            tok.type = TokenType.Plus
            tok.value = "+"
        }
        '-' => {
            tok.type = TokenType.Minus
            tok.value = "-"
        }
        '*' => {
            tok.type = TokenType.Star
            tok.value = "*"
        }
        '/' => {
            tok.type = TokenType.Slash
            tok.value = "/"
        }
        '%' => {
            tok.type = TokenType.Percent
            tok.value = "%"
        }
        '=' => {
            tok.type = TokenType.Assign
            tok.value = "="
        }
        '<' => {
            tok.type = TokenType.Less
            tok.value = "<"
        }
        '>' => {
            tok.type = TokenType.Greater
            tok.value = ">"
        }
        '!' => {
            tok.type = TokenType.Not
            tok.value = "!"
        }
        '(' => {
            tok.type = TokenType.LeftParen
            tok.value = "("
        }
        ')' => {
            tok.type = TokenType.RightParen
            tok.value = ")"
        }
        '{' => {
            tok.type = TokenType.LeftBrace
            tok.value = "{"
        }
        '}' => {
            tok.type = TokenType.RightBrace
            tok.value = "}"
        }
        '[' => {
            tok.type = TokenType.LeftBracket
            tok.value = "["
        }
        ']' => {
            tok.type = TokenType.RightBracket
            tok.value = "]"
        }
        ',' => {
            tok.type = TokenType.Comma
            tok.value = ","
        }
        ';' => {
            tok.type = TokenType.Semicolon
            tok.value = ";"
        }
        ':' => {
            tok.type = TokenType.Colon
            tok.value = ":"
        }
        '.' => {
            tok.type = TokenType.Dot
            tok.value = "."
        }
        '@' => {
            tok.type = TokenType.At
            tok.value = "@"
        }
        '#' => {
            tok.type = TokenType.Hash
            tok.value = "#"
        }
        '?' => {
            tok.type = TokenType.Question
            tok.value = "?"
        }
        '|' => {
            tok.type = TokenType.Pipe
            tok.value = "|"
        }
        _ => {
            tok.type = TokenType.Error
            tok.value = string_from_char(ch)
        }
    }
    
    read_char(lex)
    return tok
}

// Tokenize the entire input
tokenize = (input: string) [Token; 1000] {
    lex := new_lexer(input)
    tokens: [Token; 1000]
    count := 0
    
    loop {
        tok := next_token(&lex)
        tokens[count] = tok
        count = count + 1
        
        if tok.type == TokenType.Eof || count >= 1000 {
            break
        }
    }
    
    return tokens
}

// String helper functions (simplified stubs)
string_len = (s: string) i32   {
    // Stub: would need actual implementation
    return 0
}

string_char_at = (s: string, index: i32) i8   {
    // Stub: would need actual implementation  
    return 0
}

string_substring = (s: string, start: i32, end: i32) string   {
    // Stub: would need actual implementation
    return ""
}

string_contains = (s: string, substr: string) bool   {
    // Stub: would need actual implementation
    return false
}

string_from_char = (ch: i8) string   {
    // Stub: would need actual implementation
    return ""
}