// Test the self-hosted lexer implementation
// This tests basic tokenization capabilities

// Import the self-hosted lexer
lexer = @std
io = @std

// Test helper to print token info
print_token = (tok: lexer.Token) void   {
    io.print("Token: ")
    tok.token_type ?
        | lexer.TokenType.Identifier { io.print("IDENT") }
        | lexer.TokenType.Integer { io.print("INT") }
        | lexer.TokenType.Float { io.print("FLOAT") }
        | lexer.TokenType.StringLiteral { io.print("STRING") }
        | lexer.TokenType.Keyword { io.print("KEYWORD") }
        | lexer.TokenType.Symbol { io.print("SYMBOL") }
        | lexer.TokenType.Operator { io.print("OP") }
        | lexer.TokenType.Eof { io.print("EOF") }
        | _ { io.print("UNKNOWN") }
    
    io.print(" value='")
    io.print(tok.value)
    io.print("' line=")
    io.print_i32(tok.line)
    io.print(" col=")
    io.print_i32(tok.column)
    io.print("\n")
}

// Test basic tokenization
test_basic_tokens = () void   {
    io.print("=== Testing Basic Tokenization ===\n")
    
    // Test simple expression
    input := "x = 42 + 3.14"
    lex := lexer.lexer_new(input)
    
    // Read tokens
    tok1 := lexer.lexer_next_token(&lex)
    print_token(tok1)  // Should be identifier 'x'
    
    tok2 := lexer.lexer_next_token(&lex)
    print_token(tok2)  // Should be operator '='
    
    tok3 := lexer.lexer_next_token(&lex)
    print_token(tok3)  // Should be integer '42'
    
    tok4 := lexer.lexer_next_token(&lex)
    print_token(tok4)  // Should be operator '+'
    
    tok5 := lexer.lexer_next_token(&lex)
    print_token(tok5)  // Should be float '3.14'
    
    tok6 := lexer.lexer_next_token(&lex)
    print_token(tok6)  // Should be EOF
}

// Test keyword recognition
test_keywords = () void   {
    io.print("\n=== Testing Keywords ===\n")
    
    input := "if loop return struct"
    lex := lexer.lexer_new(input)
    
    // Read all tokens
    loop {
        tok := lexer.lexer_next_token(&lex)
        print_token(tok)
        
        tok.token_type ?
            | lexer.TokenType.Eof { break }
            | _ {}
    }
}

// Test string literals
test_strings = () void   {
    io.print("\n=== Testing String Literals ===\n")
    
    input := "\"hello\" \"world\\n\" \"test\""
    lex := lexer.lexer_new(input)
    
    loop {
        tok := lexer.lexer_next_token(&lex)
        print_token(tok)
        
        tok.token_type ?
            | lexer.TokenType.Eof { break }
            | _ {}
    }
}

// Test operators
test_operators = () void   {
    io.print("\n=== Testing Operators ===\n")
    
    input := "+ - * / == != < > <= >= && ||"
    lex := lexer.lexer_new(input)
    
    loop {
        tok := lexer.lexer_next_token(&lex)
        print_token(tok)
        
        tok.token_type ?
            | lexer.TokenType.Eof { break }
            | _ {}
    }
}

// Test comments
test_comments = () void   {
    io.print("\n=== Testing Comments ===\n")
    
    input := "x = 1 // This is a comment\ny = 2 /* block comment */ z = 3"
    lex := lexer.lexer_new(input)
    
    loop {
        tok := lexer.lexer_next_token(&lex)
        print_token(tok)
        
        tok.token_type ?
            | lexer.TokenType.Eof { break }
            | _ {}
    }
}

// Main test runner
main = () i32   {
    io.print("Self-Hosted Lexer Test Suite\n")
    io.print("============================\n\n")
    
    test_basic_tokens()
    test_keywords()
    test_strings()
    test_operators()
    test_comments()
    
    io.print("\nAll lexer tests completed!\n")
    return 0
}