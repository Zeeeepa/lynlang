// Zen Standard Library: Async Executor (Syscall-based)
// No FFI - uses compiler.syscall* intrinsics
//
// DESIGN: Avoiding the Red/Blue Function Problem
// =============================================
// Traditional async (Rust, JavaScript, Python) splits functions into two colors:
// - "async" functions that return futures
// - "sync" functions that block
// These can't easily call each other, creating the red/blue problem.
//
// Zen's approach: Functions are NOT inherently async or sync.
// Instead, the EXECUTOR CONTEXT determines blocking behavior:
//
//   // Same function, different execution contexts:
//   file.read(buf, allocator)           // Uses whatever context allocator provides
//   file.read(buf, blocking_allocator)  // Blocks until complete
//   file.read(buf, async_allocator)     // Returns immediately, polls later
//
// The allocator/executor carries the execution strategy, not the function.
// This is similar to Zig's async frame approach but integrated with allocators.

{ compiler } = @std
{ Option } = @std.core.option
{ Allocator } = @std.memory.allocator
{ futex_wait, futex_wake_one, futex_wake_all } = @std.sync.futex

// ============================================================================
// Task State
// ============================================================================

TASK_PENDING = 0
TASK_READY = 1
TASK_RUNNING = 2
TASK_COMPLETED = 3
TASK_CANCELLED = 4

// ============================================================================
// Task - A unit of async work
// ============================================================================
// Tasks are NOT futures in the traditional sense.
// They represent suspended work that can resume in any execution context.

Task: {
    state: i32,
    result: i64,       // Result value or error code
    waker: i64,        // Pointer to waker function
    context: i64,      // User context for the task
    next: i64          // Next task in queue (intrusive list)
}

Task.new = (context: i64) Task {
    return Task {
        state: TASK_PENDING,
        result: 0,
        waker: 0,
        context: context,
        next: 0
    }
}

Task.is_pending = (self: Ptr<Task>) bool {
    state = compiler.atomic_load(&self.val.state.ref() as Ptr<u64>) as i32
    return state == TASK_PENDING
}

Task.is_completed = (self: Ptr<Task>) bool {
    state = compiler.atomic_load(&self.val.state.ref() as Ptr<u64>) as i32
    return state == TASK_COMPLETED
}

Task.complete = (self: MutPtr<Task>, result: i64) void {
    self.val.result = result
    compiler.atomic_store(&self.val.state.ref() as Ptr<u64>, TASK_COMPLETED)
    // Wake any waiters
    self.val.waker != 0 ? {
        futex_wake_all(&self.val.state.ref())
    }
}

Task.cancel = (self: MutPtr<Task>) void {
    compiler.atomic_store(&self.val.state.ref() as Ptr<u64>, TASK_CANCELLED)
    self.val.waker != 0 ? {
        futex_wake_all(&self.val.state.ref())
    }
}

// Block until task completes (synchronous wait)
Task.wait = (self: MutPtr<Task>) i64 {
    state = compiler.atomic_load(&self.val.state.ref() as Ptr<u64>) as i32
    state < TASK_COMPLETED ? {
        futex_wait(&self.val.state.ref(), state)
    }
    return self.val.result
}

// ============================================================================
// Executor - Runs tasks (determines execution strategy)
// ============================================================================
// The executor is the key to avoiding red/blue:
// - BlockingExecutor: runs tasks synchronously
// - ThreadPoolExecutor: distributes to worker threads
// - EventLoopExecutor: uses epoll for I/O multiplexing
//
// Functions don't care which executor they run on.

Executor: behavior {
    // Submit a task for execution
    submit: (self: Self, task: MutPtr<Task>) void

    // Poll for completed tasks (non-blocking)
    poll: (self: Self) Option<MutPtr<Task>>

    // Block until a task completes
    block_on: (self: Self, task: MutPtr<Task>) i64
}

// ============================================================================
// BlockingExecutor - Simple synchronous execution
// ============================================================================
// Runs tasks immediately in the calling thread.
// No scheduling overhead, just direct execution.

BlockingExecutor: {
    _phantom: i32  // Zero-size marker
}

BlockingExecutor.new = () BlockingExecutor {
    return BlockingExecutor { _phantom: 0 }
}

BlockingExecutor.submit = (self: MutPtr<BlockingExecutor>, task: MutPtr<Task>) void {
    // In blocking mode, tasks are executed immediately
    // The task's work function should be called here
    // For now, just mark as ready (caller drives execution)
    compiler.atomic_store(&task.val.state.ref() as Ptr<u64>, TASK_READY)
}

BlockingExecutor.poll = (self: MutPtr<BlockingExecutor>) Option<MutPtr<Task>> {
    // Blocking executor doesn't queue tasks
    return Option.None()
}

BlockingExecutor.block_on = (self: MutPtr<BlockingExecutor>, task: MutPtr<Task>) i64 {
    return task.wait()
}

// ============================================================================
// TaskQueue - Simple FIFO queue for tasks
// ============================================================================

TaskQueue: {
    head: i64,  // Pointer to first task
    tail: i64,  // Pointer to last task
    count: i32
}

TaskQueue.new = () TaskQueue {
    return TaskQueue { head: 0, tail: 0, count: 0 }
}

TaskQueue.push = (self: MutPtr<TaskQueue>, task: MutPtr<Task>) void {
    task_ptr = compiler.ptr_to_int(&task.ref() as RawPtr<u8>)
    task.val.next = 0

    self.val.tail == 0 ? {
        self.val.head = task_ptr
        self.val.tail = task_ptr
    }
    self.val.tail != 0 ? {
        tail_task = compiler.int_to_ptr(self.val.tail) as MutPtr<Task>
        tail_task.val.next = task_ptr
        self.val.tail = task_ptr
    }
    self.val.count = self.val.count + 1
}

TaskQueue.pop = (self: MutPtr<TaskQueue>) Option<MutPtr<Task>> {
    self.val.head == 0 ? { return Option.None() }

    task = compiler.int_to_ptr(self.val.head) as MutPtr<Task>
    self.val.head = task.val.next
    self.val.head == 0 ? { self.val.tail = 0 }
    self.val.count = self.val.count - 1

    return Option.Some(task)
}

TaskQueue.is_empty = (self: Ptr<TaskQueue>) bool {
    return self.val.count == 0
}

// ============================================================================
// Waker - Mechanism to wake up suspended tasks
// ============================================================================
// When an async operation completes (e.g., I/O ready), the waker
// notifies the executor that a task can proceed.

Waker: {
    task: MutPtr<Task>,
    executor: i64  // Pointer to executor (type-erased)
}

Waker.new = (task: MutPtr<Task>) Waker {
    return Waker { task: task, executor: 0 }
}

Waker.wake = (self: MutPtr<Waker>) void {
    compiler.atomic_store(&self.val.task.val.state.ref() as Ptr<u64>, TASK_READY)
    futex_wake_one(&self.val.task.val.state.ref())
}

// ============================================================================
// AsyncAllocator - Allocator that carries execution context
// ============================================================================
// This is the key innovation: the ALLOCATOR determines async behavior.
// Functions receive an allocator and use whatever execution strategy it provides.
//
// Usage:
//   // Same function, different behavior based on allocator:
//   result = do_io(data, blocking_allocator)  // Blocks
//   result = do_io(data, async_allocator)     // Returns task handle
//
// This avoids function coloring entirely.

AsyncAllocator: {
    base: Allocator,
    executor: i64,       // Pointer to executor
    is_async: bool       // Whether to use async execution
}

AsyncAllocator.blocking = (base: Allocator) AsyncAllocator {
    return AsyncAllocator {
        base: base,
        executor: 0,
        is_async: false
    }
}

AsyncAllocator.async_with = (base: Allocator, executor: i64) AsyncAllocator {
    return AsyncAllocator {
        base: base,
        executor: executor,
        is_async: true
    }
}

// Allocator interface delegation
AsyncAllocator.allocate = (self: AsyncAllocator, size: usize) i64 {
    return self.base.allocate(size)
}

AsyncAllocator.deallocate = (self: AsyncAllocator, ptr: i64, size: usize) void {
    self.base.deallocate(ptr, size)
}

AsyncAllocator.reallocate = (self: AsyncAllocator, ptr: i64, old_size: usize, new_size: usize) i64 {
    return self.base.reallocate(ptr, old_size, new_size)
}

// ============================================================================
// Poll Result - For non-blocking operations
// ============================================================================

PollResult<T>:
    Ready: T,
    Pending

// ============================================================================
// Design Notes
// ============================================================================
//
// The async model works like this:
//
// 1. Functions take an AsyncAllocator instead of a plain Allocator
// 2. If is_async is false, they execute synchronously
// 3. If is_async is true, they:
//    a. Create a Task
//    b. Submit it to the executor
//    c. Return immediately (or return the Task for polling)
// 4. The executor handles scheduling and completion
//
// This means:
// - NO function coloring (async/sync keywords)
// - Execution strategy is a runtime choice
// - Same code works in sync and async contexts
// - Allocator already flows through all functions (Zig-style)
//
// Example usage pattern:
//
//   // Sync context
//   allocator = AsyncAllocator.blocking(gpa)
//   data = file.read_all(allocator)  // Blocks until complete
//
//   // Async context
//   executor = ThreadPoolExecutor.new(4)
//   allocator = AsyncAllocator.async_with(gpa, &executor)
//   task = file.read_all(allocator)  // Returns Task
//   // ... do other work ...
//   data = task.wait()               // Block when needed
