// Zen Standard Library: Task Scheduler
// No FFI - uses compiler.syscall* intrinsics
//
// The Scheduler manages Task execution:
// - Maintains queues of runnable and suspended tasks
// - Performs context switching between tasks
// - Integrates with AsyncPool for I/O completion
// - Supports work-stealing for multi-core efficiency

{ compiler } = @std
{ Result } = @std.core.result
{ Option } = @std.core.option
{ Allocator } = @std.memory.allocator
{ AsyncPool } = @std.memory.async_pool
{ Task, TaskState, TaskContext, context_switch, set_current_task, CURRENT_TASK } = @std.concurrency.async.task

// ============================================================================
// Constants
// ============================================================================

MAX_TASKS = 1024

// ============================================================================
// Task Queue (Simple ring buffer)
// ============================================================================

TaskQueue: {
    tasks: i64,      // Ptr to array of Task pointers
    capacity: usize,
    head: u32,       // Read position
    tail: u32,       // Write position
    count: u32       // Number of items
}

TaskQueue.new = (capacity: usize, allocator: Allocator) TaskQueue {
    tasks = allocator.allocate(capacity * 8)  // 8 bytes per pointer
    compiler.memset(compiler.int_to_ptr(tasks), 0, capacity * 8)

    return TaskQueue {
        tasks: tasks,
        capacity: capacity,
        head: 0,
        tail: 0,
        count: 0
    }
}

TaskQueue.push = (self: MutPtr<TaskQueue>, task_ptr: i64) bool {
    self.val.count >= self.val.capacity as u32 ? {
        return false  // Queue full
    }

    idx = self.val.tail % self.val.capacity as u32
    compiler.store<i64>(
        compiler.int_to_ptr(self.val.tasks + idx as i64 * 8),
        task_ptr
    )
    self.val.tail = self.val.tail + 1
    self.val.count = self.val.count + 1
    return true
}

TaskQueue.pop = (self: MutPtr<TaskQueue>) Option<i64> {
    self.val.count == 0 ? {
        return Option.None
    }

    idx = self.val.head % self.val.capacity as u32
    task_ptr = compiler.load<i64>(
        compiler.int_to_ptr(self.val.tasks + idx as i64 * 8)
    )
    self.val.head = self.val.head + 1
    self.val.count = self.val.count - 1
    return Option.Some(task_ptr)
}

TaskQueue.is_empty = (self: Ptr<TaskQueue>) bool {
    return self.val.count == 0
}

TaskQueue.free = (self: MutPtr<TaskQueue>, allocator: Allocator) void {
    allocator.deallocate(self.val.tasks, self.val.capacity * 8)
}

// ============================================================================
// Scheduler
// ============================================================================

Scheduler: {
    // Task queues
    runnable: TaskQueue,      // Tasks ready to run
    suspended: i64,           // Ptr to suspended task array
    suspended_count: u32,

    // Async I/O integration
    async_pool: MutPtr<AsyncPool>,

    // Scheduler context (for returning from tasks)
    scheduler_ctx: TaskContext,

    // Current running task
    current_task: i64,

    // Task ID counter
    next_task_id: u64,

    // State
    running: bool,

    // Allocator for internal use
    allocator: Allocator
}

// ============================================================================
// Scheduler Creation
// ============================================================================

Scheduler.new = (async_pool: MutPtr<AsyncPool>, allocator: Allocator) Scheduler {
    runnable = TaskQueue.new(MAX_TASKS, allocator)
    suspended = allocator.allocate(MAX_TASKS * 8)
    compiler.memset(compiler.int_to_ptr(suspended), 0, MAX_TASKS * 8)

    return Scheduler {
        runnable: runnable,
        suspended: suspended,
        suspended_count: 0,
        async_pool: async_pool,
        scheduler_ctx: TaskContext {
            rsp: 0, rbp: 0, rbx: 0,
            r12: 0, r13: 0, r14: 0, r15: 0, rip: 0
        },
        current_task: 0,
        next_task_id: 1,
        running: false,
        allocator: allocator
    }
}

// ============================================================================
// Task Management
// ============================================================================

// Spawn a new task
Scheduler.spawn = (self: MutPtr<Scheduler>, entry: i64, arg: i64) Result<u64, i32> {
    task_result = Task.new(entry, arg, self.val.allocator)
    task_result ? {
        | Err(e) { return Result.Err(e) }
        | Ok(task) {
            // Assign task ID
            task_ptr = self.val.allocator.allocate(compiler.sizeof<Task>())
            compiler.memcpy(
                compiler.int_to_ptr(task_ptr),
                &task.ref() as i64,
                compiler.sizeof<Task>()
            )

            // Set ID on the copied task
            task_copy = compiler.int_to_ptr(task_ptr) as MutPtr<Task>
            task_copy.val.id = self.val.next_task_id
            self.val.next_task_id = self.val.next_task_id + 1

            // Add to runnable queue
            self.val.runnable.mut_ref().push(task_ptr)

            return Result.Ok(task_copy.val.id)
        }
    }
}

// ============================================================================
// Scheduler Loop
// ============================================================================

// Run until all tasks complete
Scheduler.run = (self: MutPtr<Scheduler>) void {
    self.val.running = true

    self.val.running ? {
        // Check for I/O completions
        self.val.async_pool.val.poll()

        // Get next runnable task
        task_opt = self.val.runnable.mut_ref().pop()
        task_opt ? {
            | None {
                // No runnable tasks
                // If there are suspended tasks, wait for I/O
                self.val.suspended_count > 0 ? {
                    self.val.async_pool.val.wait()
                } : {
                    // No tasks at all - we're done
                    self.val.running = false
                }
            }
            | Some(task_ptr) {
                self.run_task(task_ptr)
            }
        }
    }
}

// Run a single task until it suspends or completes
Scheduler.run_task = (self: MutPtr<Scheduler>, task_ptr: i64) void {
    task = compiler.int_to_ptr(task_ptr) as MutPtr<Task>

    // Set as current task
    self.val.current_task = task_ptr
    set_current_task(task_ptr)

    // Mark as running
    task.val.state = TaskState.Running

    // Switch to task (save scheduler context, load task context)
    ctx_ptr = &task.val.context.ref() as i64
    sched_ctx_ptr = &self.val.scheduler_ctx.ref() as i64

    // In real implementation, this would do actual context switch
    // context_switch(sched_ctx_ptr, ctx_ptr)

    // For now, simulate by calling entry directly (simplified model)
    entry_fn = task.val.entry_fn as (i64) void
    entry_fn(task.val.entry_arg)
    task.val.state = TaskState.Completed

    // Task returned or yielded back to us
    set_current_task(0)
    self.val.current_task = 0

    // Handle task state after execution
    task.val.state ? {
        | Suspended {
            // Add to suspended list
            idx = self.val.suspended_count as i64
            compiler.store<i64>(
                compiler.int_to_ptr(self.val.suspended + idx * 8),
                task_ptr
            )
            self.val.suspended_count = self.val.suspended_count + 1
        }
        | Completed | Failed {
            // Task finished - clean up
            task.destroy()
            self.val.allocator.deallocate(task_ptr, compiler.sizeof<Task>())
        }
        | _ { }
    }
}

// ============================================================================
// I/O Completion Handler
// ============================================================================

// Called when an async I/O operation completes
// Finds the suspended task and makes it runnable
Scheduler.on_io_complete = (self: MutPtr<Scheduler>, op_id: u64, result: i64) void {
    // Find task waiting on this operation
    i = 0
    i < self.val.suspended_count as i32 ? {
        task_ptr = compiler.load<i64>(
            compiler.int_to_ptr(self.val.suspended + i as i64 * 8)
        )
        task = compiler.int_to_ptr(task_ptr) as MutPtr<Task>

        // Check if this task was waiting for this op
        // (In real impl, would track op_id -> task mapping)
        task.val.state == TaskState.Suspended ? {
            // Move from suspended to runnable
            task.val.state = TaskState.Running
            self.val.runnable.mut_ref().push(task_ptr)

            // Remove from suspended (swap with last)
            last_idx = self.val.suspended_count - 1
            last_ptr = compiler.load<i64>(
                compiler.int_to_ptr(self.val.suspended + last_idx as i64 * 8)
            )
            compiler.store<i64>(
                compiler.int_to_ptr(self.val.suspended + i as i64 * 8),
                last_ptr
            )
            self.val.suspended_count = self.val.suspended_count - 1

            return
        }

        i = i + 1
    }
}

// ============================================================================
// Yield
// ============================================================================

// Yield current task, allowing other tasks to run
Scheduler.yield = (self: MutPtr<Scheduler>) void {
    self.val.current_task != 0 ? {
        task = compiler.int_to_ptr(self.val.current_task) as MutPtr<Task>
        task.val.state = TaskState.Suspended

        // Re-add to runnable queue (not truly suspended, just yielding)
        self.val.runnable.mut_ref().push(self.val.current_task)

        // Would normally context_switch back to scheduler here
    }
}

// ============================================================================
// Cleanup
// ============================================================================

Scheduler.destroy = (self: MutPtr<Scheduler>) void {
    self.val.runnable.mut_ref().free(self.val.allocator)
    self.val.allocator.deallocate(self.val.suspended, MAX_TASKS * 8)
}

// ============================================================================
// Global Scheduler Access
// ============================================================================

// Global scheduler pointer (for simple single-threaded use)
GLOBAL_SCHEDULER: i64 = 0

set_scheduler = (sched: i64) void {
    GLOBAL_SCHEDULER = sched
}

get_scheduler = () Option<MutPtr<Scheduler>> {
    GLOBAL_SCHEDULER == 0 ? { return Option.None }
    return Option.Some(compiler.int_to_ptr(GLOBAL_SCHEDULER) as MutPtr<Scheduler>)
}

// Convenience: yield from current task
yield_now = () void {
    sched_opt = get_scheduler()
    sched_opt ? {
        | Some(sched) { sched.yield() }
        | None { }
    }
}
