// Zen Standard Library: Atomic Types (Syscall-based)
// No FFI - uses compiler.atomic_* intrinsics
// Type-safe atomic primitives for lock-free programming

{ compiler } = @std
{ Option } = @std.core.option

// ============================================================================
// Memory Ordering
// ============================================================================
// These map to LLVM atomics ordering semantics

ORDERING_RELAXED = 0    // No synchronization
ORDERING_ACQUIRE = 1    // Acquire semantics (loads)
ORDERING_RELEASE = 2    // Release semantics (stores)
ORDERING_ACQ_REL = 3    // Acquire-Release (RMW)
ORDERING_SEQ_CST = 4    // Sequential consistency (strongest)

// ============================================================================
// AtomicBool
// ============================================================================

AtomicBool: {
    value: i32  // 0 = false, 1 = true
}

AtomicBool.new = (initial: bool) AtomicBool {
    val = initial ? { 1 } : { 0 }
    return AtomicBool { value: val }
}

AtomicBool.load = (self: Ptr<AtomicBool>) bool {
    val = compiler.atomic_load(&self.val.value.ref() as Ptr<u64>) as i32
    return val != 0
}

AtomicBool.store = (self: MutPtr<AtomicBool>, value: bool) void {
    val = value ? { 1 } : { 0 }
    compiler.atomic_store(&self.val.value.ref() as Ptr<u64>, val as u64)
}

// Swap and return old value
AtomicBool.swap = (self: MutPtr<AtomicBool>, value: bool) bool {
    new_val = value ? { 1 } : { 0 }
    old = compiler.atomic_xchg(&self.val.value.ref() as Ptr<u64>, new_val as u64) as i32
    return old != 0
}

// Compare and swap: if current == expected, set to desired
// Returns true if swap succeeded
AtomicBool.compare_exchange = (self: MutPtr<AtomicBool>, expected: bool, desired: bool) bool {
    exp = expected ? { 1 } : { 0 }
    des = desired ? { 1 } : { 0 }
    old = compiler.atomic_cas(&self.val.value.ref() as Ptr<u64>, exp as u64, des as u64) as i32
    return old == exp
}

// Fetch and set to true, return old value
AtomicBool.fetch_or = (self: MutPtr<AtomicBool>, value: bool) bool {
    value == false ? { return self.load() }
    old = compiler.atomic_xchg(&self.val.value.ref() as Ptr<u64>, 1) as i32
    return old != 0
}

// ============================================================================
// AtomicI32
// ============================================================================

AtomicI32: {
    value: i32
}

AtomicI32.new = (initial: i32) AtomicI32 {
    return AtomicI32 { value: initial }
}

AtomicI32.load = (self: Ptr<AtomicI32>) i32 {
    return compiler.atomic_load(&self.val.value.ref() as Ptr<u64>) as i32
}

AtomicI32.store = (self: MutPtr<AtomicI32>, value: i32) void {
    compiler.atomic_store(&self.val.value.ref() as Ptr<u64>, value as u64)
}

AtomicI32.swap = (self: MutPtr<AtomicI32>, value: i32) i32 {
    return compiler.atomic_xchg(&self.val.value.ref() as Ptr<u64>, value as u64) as i32
}

AtomicI32.compare_exchange = (self: MutPtr<AtomicI32>, expected: i32, desired: i32) bool {
    old = compiler.atomic_cas(&self.val.value.ref() as Ptr<u64>, expected as u64, desired as u64) as i32
    return old == expected
}

// Returns old value, adds delta
AtomicI32.fetch_add = (self: MutPtr<AtomicI32>, delta: i32) i32 {
    return compiler.atomic_add(&self.val.value.ref() as Ptr<u64>, delta as u64) as i32
}

AtomicI32.fetch_sub = (self: MutPtr<AtomicI32>, delta: i32) i32 {
    return compiler.atomic_sub(&self.val.value.ref() as Ptr<u64>, delta as u64) as i32
}

// Convenience: add and return new value
AtomicI32.add = (self: MutPtr<AtomicI32>, delta: i32) i32 {
    old = self.fetch_add(delta)
    return old + delta
}

// Convenience: subtract and return new value
AtomicI32.sub = (self: MutPtr<AtomicI32>, delta: i32) i32 {
    old = self.fetch_sub(delta)
    return old - delta
}

// Increment and return new value
AtomicI32.inc = (self: MutPtr<AtomicI32>) i32 {
    return self.add(1)
}

// Decrement and return new value
AtomicI32.dec = (self: MutPtr<AtomicI32>) i32 {
    return self.sub(1)
}

// ============================================================================
// AtomicI64
// ============================================================================

AtomicI64: {
    value: i64
}

AtomicI64.new = (initial: i64) AtomicI64 {
    return AtomicI64 { value: initial }
}

AtomicI64.load = (self: Ptr<AtomicI64>) i64 {
    return compiler.atomic_load(&self.val.value.ref() as Ptr<u64>) as i64
}

AtomicI64.store = (self: MutPtr<AtomicI64>, value: i64) void {
    compiler.atomic_store(&self.val.value.ref() as Ptr<u64>, value as u64)
}

AtomicI64.swap = (self: MutPtr<AtomicI64>, value: i64) i64 {
    return compiler.atomic_xchg(&self.val.value.ref() as Ptr<u64>, value as u64) as i64
}

AtomicI64.compare_exchange = (self: MutPtr<AtomicI64>, expected: i64, desired: i64) bool {
    old = compiler.atomic_cas(&self.val.value.ref() as Ptr<u64>, expected as u64, desired as u64) as i64
    return old == expected
}

AtomicI64.fetch_add = (self: MutPtr<AtomicI64>, delta: i64) i64 {
    return compiler.atomic_add(&self.val.value.ref() as Ptr<u64>, delta as u64) as i64
}

AtomicI64.fetch_sub = (self: MutPtr<AtomicI64>, delta: i64) i64 {
    return compiler.atomic_sub(&self.val.value.ref() as Ptr<u64>, delta as u64) as i64
}

AtomicI64.add = (self: MutPtr<AtomicI64>, delta: i64) i64 {
    return self.fetch_add(delta) + delta
}

AtomicI64.sub = (self: MutPtr<AtomicI64>, delta: i64) i64 {
    return self.fetch_sub(delta) - delta
}

AtomicI64.inc = (self: MutPtr<AtomicI64>) i64 {
    return self.add(1)
}

AtomicI64.dec = (self: MutPtr<AtomicI64>) i64 {
    return self.sub(1)
}

// ============================================================================
// AtomicU32
// ============================================================================

AtomicU32: {
    value: u32
}

AtomicU32.new = (initial: u32) AtomicU32 {
    return AtomicU32 { value: initial }
}

AtomicU32.load = (self: Ptr<AtomicU32>) u32 {
    return compiler.atomic_load(&self.val.value.ref() as Ptr<u64>) as u32
}

AtomicU32.store = (self: MutPtr<AtomicU32>, value: u32) void {
    compiler.atomic_store(&self.val.value.ref() as Ptr<u64>, value as u64)
}

AtomicU32.swap = (self: MutPtr<AtomicU32>, value: u32) u32 {
    return compiler.atomic_xchg(&self.val.value.ref() as Ptr<u64>, value as u64) as u32
}

AtomicU32.compare_exchange = (self: MutPtr<AtomicU32>, expected: u32, desired: u32) bool {
    old = compiler.atomic_cas(&self.val.value.ref() as Ptr<u64>, expected as u64, desired as u64) as u32
    return old == expected
}

AtomicU32.fetch_add = (self: MutPtr<AtomicU32>, delta: u32) u32 {
    return compiler.atomic_add(&self.val.value.ref() as Ptr<u64>, delta as u64) as u32
}

AtomicU32.fetch_sub = (self: MutPtr<AtomicU32>, delta: u32) u32 {
    return compiler.atomic_sub(&self.val.value.ref() as Ptr<u64>, delta as u64) as u32
}

// ============================================================================
// AtomicU64
// ============================================================================

AtomicU64: {
    value: u64
}

AtomicU64.new = (initial: u64) AtomicU64 {
    return AtomicU64 { value: initial }
}

AtomicU64.load = (self: Ptr<AtomicU64>) u64 {
    return compiler.atomic_load(&self.val.value.ref() as Ptr<u64>)
}

AtomicU64.store = (self: MutPtr<AtomicU64>, value: u64) void {
    compiler.atomic_store(&self.val.value.ref() as Ptr<u64>, value)
}

AtomicU64.swap = (self: MutPtr<AtomicU64>, value: u64) u64 {
    return compiler.atomic_xchg(&self.val.value.ref() as Ptr<u64>, value)
}

AtomicU64.compare_exchange = (self: MutPtr<AtomicU64>, expected: u64, desired: u64) bool {
    old = compiler.atomic_cas(&self.val.value.ref() as Ptr<u64>, expected, desired)
    return old == expected
}

AtomicU64.fetch_add = (self: MutPtr<AtomicU64>, delta: u64) u64 {
    return compiler.atomic_add(&self.val.value.ref() as Ptr<u64>, delta)
}

AtomicU64.fetch_sub = (self: MutPtr<AtomicU64>, delta: u64) u64 {
    return compiler.atomic_sub(&self.val.value.ref() as Ptr<u64>, delta)
}

// ============================================================================
// AtomicPtr<T> - Atomic pointer
// ============================================================================

AtomicPtr<T>: {
    ptr: i64  // Stored as integer for atomic ops
}

AtomicPtr.new = (initial: Ptr<T>) AtomicPtr<T> {
    return AtomicPtr<T> { ptr: compiler.ptr_to_int(&initial.ref() as RawPtr<u8>) }
}

AtomicPtr.null = () AtomicPtr<T> {
    return AtomicPtr<T> { ptr: 0 }
}

AtomicPtr.load = (self: Ptr<AtomicPtr<T>>) i64 {
    return compiler.atomic_load(&self.val.ptr.ref() as Ptr<u64>) as i64
}

AtomicPtr.store = (self: MutPtr<AtomicPtr<T>>, ptr: i64) void {
    compiler.atomic_store(&self.val.ptr.ref() as Ptr<u64>, ptr as u64)
}

AtomicPtr.swap = (self: MutPtr<AtomicPtr<T>>, ptr: i64) i64 {
    return compiler.atomic_xchg(&self.val.ptr.ref() as Ptr<u64>, ptr as u64) as i64
}

AtomicPtr.compare_exchange = (self: MutPtr<AtomicPtr<T>>, expected: i64, desired: i64) bool {
    old = compiler.atomic_cas(&self.val.ptr.ref() as Ptr<u64>, expected as u64, desired as u64) as i64
    return old == expected
}

AtomicPtr.is_null = (self: Ptr<AtomicPtr<T>>) bool {
    return self.load() == 0
}

// ============================================================================
// Memory Fence
// ============================================================================

// Full memory barrier (sequential consistency)
fence = () void {
    compiler.atomic_fence()
}

// ============================================================================
// SpinWait - Efficient spinning with backoff
// ============================================================================

SpinWait: {
    count: i32
}

SpinWait.new = () SpinWait {
    return SpinWait { count: 0 }
}

// Returns true if should yield to OS
SpinWait.spin_once = (self: MutPtr<SpinWait>) bool {
    self.val.count = self.val.count + 1

    // Spin for a while before yielding
    self.val.count < 10 ? {
        // CPU pause instruction would go here
        // For now, just increment count
        return false
    }

    // After many spins, suggest yielding
    return self.val.count > 20
}

SpinWait.reset = (self: MutPtr<SpinWait>) void {
    self.val.count = 0
}

SpinWait.spin_count = (self: Ptr<SpinWait>) i32 {
    return self.val.count
}
