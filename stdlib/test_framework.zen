// Zen Standard Library - Test Framework
// Provides testing infrastructure for Zen programs

core = @std
io = @std
assert = @std
{ Vec, DynVec } = @std
string = @std

// TestResult represents the outcome of a test
TestResult: Passed
    | Failed(message: string)
    | Skipped(reason: string)
    | Panicked(error: string)

// TestCase represents a single test
TestCase: {
    name: string,
    test_fn: () void,
    should_panic: bool,
    skip: bool,
    skip_reason: string,
}

// TestSuite represents a collection of tests
TestSuite: {
    name: string,
    tests: DynVec<TestCase>,
    setup: Option<() void>,
    teardown: Option<() void>,
    results: DynVec<(string, TestResult)>,
}

// TestRunner manages test execution
TestRunner: {
    suites: DynVec<TestSuite>,
    total_tests: u32,
    passed: u32,
    failed: u32,
    skipped: u32,
    verbose: bool,
}

// Create a new test suite
suite_new = (name: string) TestSuite   {
    TestSuite{
        name: name,
        tests: DynVec.new<TestCase>(),
        setup: None,
        teardown: None,
        results: DynVec.new<(string, TestResult)>(),
    }
}

// Add a test to the suite
suite_add_test = (suite: Ptr<TestSuite>, name: string, test_fn: () void) void {
    test := TestCase{
        name: name,
        test_fn: test_fn,
        should_panic: false,
        skip: false,
        skip_reason: "",
    }
    suite.tests.push(test)
}

// Add a test that should panic
suite_add_panic_test = (suite: Ptr<TestSuite>, name: string, test_fn: () void) void {
    test := TestCase{
        name: name,
        test_fn: test_fn,
        should_panic: true,
        skip: false,
        skip_reason: "",
    }
    suite.tests.push(test)
}

// Skip a test
suite_skip_test = (suite: Ptr<TestSuite>, name: string, reason: string, test_fn: () void) void {
    test := TestCase{
        name: name,
        test_fn: test_fn,
        should_panic: false,
        skip: true,
        skip_reason: reason,
    }
    suite.tests.push(test)
}

// Set setup function for suite
suite_setup = (suite: Ptr<TestSuite>, setup_fn: () void) void {
    suite.setup = Some(setup_fn)
}

// Set teardown function for suite
suite_teardown = (suite: Ptr<TestSuite>, teardown_fn: () void) void {
    suite.teardown = Some(teardown_fn)
}

// Run a single test case
run_test = (test: TestCase) TestResult   {
    // Check if test should be skipped
    test.skip ?
        | true { return TestResult.Skipped(test.skip_reason) }
        | false {}
    
    // Try to catch panics (simplified - real implementation would use proper panic handling)
    // For now, we'll just run the test and assume success if it doesn't crash
    
    test.should_panic ?
        | true {
            // Test should panic - in real implementation would catch panic
            test.test_fn()
            // If we get here, test didn't panic when it should have
            return TestResult.Failed("Expected test to panic but it didn't")
        }
        | false {
            // Normal test execution
            test.test_fn()
            return TestResult.Passed
        }
}

// Run all tests in a suite
run_suite = (suite: Ptr<TestSuite>) void   {
    io.println("\n=== Running test suite: $(suite.name) ===")
    
    // Run setup if provided
    suite.setup ?
        | Some(setup_fn) {
            io.println("  Running setup...")
            setup_fn()
        }
        | None {}
    
    // Run each test
    range(0, suite.tests.len()).loop(i -> {
        test := suite.tests.at(i)
        io.print("  Testing $(test.name)... ")
        
        result := run_test(test)
        
        result ?
            | TestResult.Passed {
                io.println("✓ PASSED")
                suite.results.push((test.name, result))
            }
            | TestResult.Failed(msg) {
                io.println("✗ FAILED: $(msg)")
                suite.results.push((test.name, result))
            }
            | TestResult.Skipped(reason) {
                io.println("⊘ SKIPPED: $(reason)")
                suite.results.push((test.name, result))
            }
            | TestResult.Panicked(err) {
                io.println("✗ PANICKED: $(err)")
                suite.results.push((test.name, result))
            }
    })
    
    // Run teardown if provided
    suite.teardown ?
        | Some(teardown_fn) {
            io.println("  Running teardown...")
            teardown_fn()
        }
        | None {}
}

// Create a new test runner
runner_new = () TestRunner   {
    TestRunner{
        suites: DynVec.new<TestSuite>(),
        total_tests: 0,
        passed: 0,
        failed: 0,
        skipped: 0,
        verbose: false,
    }
}

// Add a suite to the runner
runner_add_suite = (runner: Ptr<TestRunner>, suite: TestSuite) void   {
    runner.suites.push(suite)
    runner.total_tests = runner.total_tests + suite.tests.len()
}

// Enable verbose output
runner_verbose = (runner: Ptr<TestRunner>, verbose: bool) void   {
    runner.verbose = verbose
}

// Run all test suites
runner_run = (runner: Ptr<TestRunner>) bool   {
    io.println("\n" + string_repeat("=", 60))
    io.println("Running $(runner.total_tests) tests in $(runner.suites.len()) suites")
    io.println(string_repeat("=", 60))
    
    // Run each suite
    range(0, runner.suites.len()).loop(i -> {
        suite := runner.suites.at(i)
        run_suite(ptr_of(suite))
        
        // Count results
        range(0, suite.results.len()).loop(j -> {
            result_pair := suite.results.at(j)
            result := result_pair.1
            
            result ?
                | TestResult.Passed { runner.passed = runner.passed + 1 }
                | TestResult.Failed(_) { runner.failed = runner.failed + 1 }
                | TestResult.Skipped(_) { runner.skipped = runner.skipped + 1 }
                | TestResult.Panicked(_) { runner.failed = runner.failed + 1 }
        })
    })
    
    // Print summary
    io.println("\n" + string_repeat("=", 60))
    io.println("Test Summary:")
    io.println("  Total:   $(runner.total_tests)")
    io.println("  Passed:  $(runner.passed)")
    io.println("  Failed:  $(runner.failed)")
    io.println("  Skipped: $(runner.skipped)")
    
    pass_rate := (runner.passed * 100) / runner.total_tests
    io.println("  Pass Rate: $(pass_rate)%")
    
    runner.failed == 0 ?
        | true {
            io.println("\n✓ All tests passed!")
            io.println(string_repeat("=", 60))
            return true
        }
        | false {
            io.println("\n✗ Some tests failed!")
            io.println(string_repeat("=", 60))
            return false
        }
}

// Assertion helpers for tests
assert_eq = <T>(actual: T, expected: T, message: string = "") void {
    actual == expected ?
        | false {
            msg := message.empty() ?
                | true { "Expected $(expected), got $(actual)" }
                | false { message }
            panic(msg)
        }
        | true {}
}

assert_ne = <T>(actual: T, expected: T, message: string = "") void {
    actual != expected ?
        | false {
            msg := message.empty() ?
                | true { "Expected values to be different, both were $(actual)" }
                | false { message }
            panic(msg)
        }
        | true {}
}

assert_true = (value: bool, message: string = "") void   {
    value ?
        | false {
            msg := message.empty() ?
                | true { "Expected true, got false" }
                | false { message }
            panic(msg)
        }
        | true {}
}

assert_false = (value: bool, message: string = "") void   {
    value ?
        | true {
            msg := message.empty() ?
                | true { "Expected false, got true" }
                | false { message }
            panic(msg)
        }
        | false {}
}

assert_lt = <T>(a: T, b: T, message: string = "") void {
    a < b ?
        | false {
            msg := message.empty() ?
                | true { "Expected $(a) < $(b)" }
                | false { message }
            panic(msg)
        }
        | true {}
}

assert_le = <T>(a: T, b: T, message: string = "") void {
    a <= b ?
        | false {
            msg := message.empty() ?
                | true { "Expected $(a) <= $(b)" }
                | false { message }
            panic(msg)
        }
        | true {}
}

assert_gt = <T>(a: T, b: T, message: string = "") void {
    a > b ?
        | false {
            msg := message.empty() ?
                | true { "Expected $(a) > $(b)" }
                | false { message }
            panic(msg)
        }
        | true {}
}

assert_ge = <T>(a: T, b: T, message: string = "") void {
    a >= b ?
        | false {
            msg := message.empty() ?
                | true { "Expected $(a) >= $(b)" }
                | false { message }
            panic(msg)
        }
        | true {}
}

// Benchmark support
Benchmark: {
    name: string,
    fn: () void,
    iterations: u32,
    warmup: u32,
    times: DynVec<u64>,
}

// Create a new benchmark
benchmark_new = (name: string, fn: () void) Benchmark {
    Benchmark{
        name: name,
        fn: fn,
        iterations: 1000,
        warmup: 100,
        times: DynVec.new<u64>(),
    }
}

// Run a benchmark
benchmark_run = (bench: Ptr<Benchmark>) void   {
    io.println("\nRunning benchmark: $(bench.name)")
    
    // Warmup
    io.print("  Warming up... ")
    range(0, bench.warmup).loop(i -> {
        bench.fn()
    })
    io.println("done")
    
    // Actual benchmark
    io.print("  Running $(bench.iterations) iterations... ")
    
    range(0, bench.iterations).loop(i -> {
        start := time_now()
        bench.fn()
        end := time_now()
        
        elapsed := end - start
        bench.times.push(elapsed)
    })
    
    io.println("done")
    
    // Calculate statistics
    total := 0u64
    min := bench.times.at(0)
    max := bench.times.at(0)
    
    range(0, bench.times.len()).loop(i -> {
        time := bench.times.at(i)
        total = total + time
        
        time < min ?
            | true { min = time }
            | false {}
        
        time > max ?
            | true { max = time }
            | false {}
    })
    
    avg := total / bench.iterations
    
    io.println("  Results:")
    io.println("    Average: $(avg) ns")
    io.println("    Min:     $(min) ns")
    io.println("    Max:     $(max) ns")
}

// Test macro helpers
// Note: Test and benchmark registration happens via attributes at compile-time,
// not through comptime blocks. The compiler handles @std.test and @std.bench attributes.

// Register test function (compile-time attribute)
@std.test
test_register = (name: string, fn: () void) void {
    // The compiler processes @std.test attributes during compilation
}

// Register benchmark (compile-time attribute)
@std.bench
bench_register = (name: string, fn: () void) void {
    // The compiler processes @std.bench attributes during compilation
}

// Helper to get current time in nanoseconds (stub)
time_now = () u64   {
    // In real implementation, would call system time function
    return 0
}

// Helper to repeat a string
string_repeat = (s: string, n: u32) string   {
    result := ""
    range(0, n).loop(i -> {
        result = result + s
    })
    return result
}