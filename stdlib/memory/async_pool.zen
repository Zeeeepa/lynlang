// Zen Standard Library: AsyncPool Allocator
// No FFI - uses compiler.syscall* intrinsics
//
// AsyncPool is the async counterpart to GPA. When code receives an AsyncPool,
// I/O operations use io_uring for non-blocking execution.
//
// Usage:
//   async_alloc = AsyncPool.init()
//   @this.defer(async_alloc.deinit())
//
//   // Pass to functions - they automatically become async
//   result = fetch_data(url, async_alloc)

{ compiler } = @std
{ Result } = @std.core.result
{ Option } = @std.core.option
{ Allocator } = @std.memory.allocator
{ AsyncAllocator, ExecutionMode, CompletionFn } = @std.memory.async_allocator
{ IoUring, CQE } = @std.io.uring
{ SYS_MMAP, SYS_MUNMAP } = @std.sys.syscall

// ============================================================================
// Constants
// ============================================================================

// Default io_uring queue size
DEFAULT_RING_SIZE = 256

// Default memory pool size (4MB)
DEFAULT_POOL_SIZE = 4194304

// Max pending callbacks
MAX_PENDING_OPS = 1024

// ============================================================================
// Pending Operation Tracking
// ============================================================================

PendingOp: {
    user_data: u64,
    callback: CompletionFn,
    active: bool
}

// ============================================================================
// AsyncPool - Async Memory Allocator
// ============================================================================

AsyncPool: {
    // Memory pool (bump allocator)
    pool_base: i64,
    pool_size: usize,
    pool_offset: u64,  // Current allocation offset

    // io_uring instance for async I/O
    ring: IoUring,

    // Pending operation tracking
    pending_ops: i64,      // Ptr to PendingOp array
    next_op_id: u64,       // Counter for operation IDs
    pending_count: i32     // Number of pending operations
}

// ============================================================================
// AsyncPool Creation
// ============================================================================

AsyncPool.init = () Result<AsyncPool, i32> {
    return AsyncPool.init_with_size(DEFAULT_POOL_SIZE, DEFAULT_RING_SIZE)
}

AsyncPool.init_with_size = (pool_size: usize, ring_size: u32) Result<AsyncPool, i32> {
    // Allocate memory pool using mmap
    pool_base = compiler.syscall6(
        9,  // SYS_MMAP
        0,
        pool_size,
        3,  // PROT_READ | PROT_WRITE
        34, // MAP_PRIVATE | MAP_ANONYMOUS
        -1,
        0
    )
    pool_base < 0 ? {
        return Result.Err((0 - pool_base) as i32)
    }

    // Create io_uring
    ring_result = IoUring.new(ring_size)
    ring_result ? {
        | Err(e) {
            compiler.syscall2(11, pool_base, pool_size)  // munmap
            return Result.Err(e)
        }
        | Ok(ring) {
            // Allocate pending ops array
            ops_size = MAX_PENDING_OPS * 24  // sizeof(PendingOp) ~= 24
            ops_ptr = compiler.raw_allocate(ops_size)
            compiler.memset(compiler.int_to_ptr(ops_ptr), 0, ops_size)

            pool = AsyncPool {
                pool_base: pool_base,
                pool_size: pool_size,
                pool_offset: 0,
                ring: ring,
                pending_ops: ops_ptr,
                next_op_id: 1,
                pending_count: 0
            }

            return Result.Ok(pool)
        }
    }
}

// ============================================================================
// AsyncAllocator Implementation
// ============================================================================

AsyncPool.implements(AsyncAllocator, {
    // Memory allocation from pool (bump allocator)
    allocate = (self: AsyncPool, size: usize) i64 {
        // Align to 8 bytes
        aligned_size = (size + 7) & ~7

        // Check if pool has space
        new_offset = self.pool_offset + aligned_size as u64
        new_offset > self.pool_size as u64 ? {
            return 0  // Out of memory
        }

        // Bump allocate
        ptr = self.pool_base + self.pool_offset as i64
        self.pool_offset = new_offset
        return ptr
    },

    deallocate = (self: AsyncPool, ptr: i64, size: usize) void {
        // Bump allocator doesn't deallocate individual allocations
        // Memory is freed when pool is destroyed
    },

    reallocate = (self: AsyncPool, ptr: i64, old_size: usize, new_size: usize) i64 {
        // For bump allocator, just allocate new and copy
        new_ptr = self.allocate(new_size)
        new_ptr == 0 ? { return 0 }

        copy_size = old_size < new_size ? { old_size } : { new_size }
        compiler.memcpy(compiler.int_to_ptr(new_ptr), compiler.int_to_ptr(ptr), copy_size)

        return new_ptr
    },

    mode = (self: AsyncPool) ExecutionMode {
        return ExecutionMode.Async
    },

    // Schedule async read via io_uring
    schedule_read = (self: AsyncPool, fd: i32, buf: i64, len: usize, offset: i64, callback: CompletionFn, user_data: u64) i64 {
        self_mut = &self.mut_ref()
        op_id = self_mut.alloc_pending_op(callback, user_data)

        result = self_mut.val.ring.mut_ref().prep_read(fd, buf, len as u32, offset as u64, op_id)
        result ? {
            | Err(e) { return 0 - e as i64 }
            | Ok(_) {
                self_mut.val.ring.mut_ref().submit()
                self_mut.val.pending_count = self_mut.val.pending_count + 1
                return op_id as i64
            }
        }
    },

    schedule_write = (self: AsyncPool, fd: i32, buf: i64, len: usize, offset: i64, callback: CompletionFn, user_data: u64) i64 {
        self_mut = &self.mut_ref()
        op_id = self_mut.alloc_pending_op(callback, user_data)

        result = self_mut.val.ring.mut_ref().prep_write(fd, buf, len as u32, offset as u64, op_id)
        result ? {
            | Err(e) { return 0 - e as i64 }
            | Ok(_) {
                self_mut.val.ring.mut_ref().submit()
                self_mut.val.pending_count = self_mut.val.pending_count + 1
                return op_id as i64
            }
        }
    },

    schedule_accept = (self: AsyncPool, listen_fd: i32, addr: i64, addrlen: i64, callback: CompletionFn, user_data: u64) i64 {
        self_mut = &self.mut_ref()
        op_id = self_mut.alloc_pending_op(callback, user_data)

        result = self_mut.val.ring.mut_ref().prep_accept(listen_fd, addr, addrlen, op_id)
        result ? {
            | Err(e) { return 0 - e as i64 }
            | Ok(_) {
                self_mut.val.ring.mut_ref().submit()
                self_mut.val.pending_count = self_mut.val.pending_count + 1
                return op_id as i64
            }
        }
    },

    schedule_connect = (self: AsyncPool, fd: i32, addr: i64, addrlen: i32, callback: CompletionFn, user_data: u64) i64 {
        self_mut = &self.mut_ref()
        op_id = self_mut.alloc_pending_op(callback, user_data)

        // Submit connect SQE manually (IORING_OP_CONNECT = 16)
        sqe_ptr = self_mut.val.ring.mut_ref().get_sqe()
        sqe_ptr == 0 ? { return -11 }  // EAGAIN

        compiler.store<u8>(compiler.int_to_ptr(sqe_ptr), 16)      // IORING_OP_CONNECT
        compiler.store<i32>(compiler.int_to_ptr(sqe_ptr + 4), fd)
        compiler.store<u64>(compiler.int_to_ptr(sqe_ptr + 16), addr as u64)
        compiler.store<u32>(compiler.int_to_ptr(sqe_ptr + 8), addrlen as u32)
        compiler.store<u64>(compiler.int_to_ptr(sqe_ptr + 32), op_id)

        self_mut.val.ring.mut_ref().submit_sqe(sqe_ptr)
        self_mut.val.ring.mut_ref().submit()
        self_mut.val.pending_count = self_mut.val.pending_count + 1

        return op_id as i64
    },

    // Poll for completions without blocking
    poll = (self: AsyncPool) i32 {
        completed = 0
        self_mut = &self.mut_ref()

        // Check for CQEs
        cqe = self_mut.val.ring.mut_ref().peek_cqe()
        cqe ? {
            | None { }
            | Some(c) {
                self_mut.process_cqe(c)
                self_mut.val.ring.mut_ref().cqe_seen()
                completed = completed + 1
            }
        }

        return completed
    },

    // Wait for at least one completion
    wait = (self: AsyncPool) i32 {
        self_mut = &self.mut_ref()

        // First check if any completions ready
        completed = self_mut.val.poll()
        completed > 0 ? { return completed }

        // Block waiting for completion
        result = self_mut.val.ring.mut_ref().wait_cqe()
        result ? {
            | Err(e) { return 0 - e }
            | Ok(cqe) {
                self_mut.process_cqe(cqe)
                self_mut.val.ring.mut_ref().cqe_seen()
                return 1
            }
        }
    },

    cancel = (self: AsyncPool, op_id: u64) bool {
        slot = (op_id - 1) % MAX_PENDING_OPS as u64
        op_ptr = self.pending_ops + slot as i64 * 24

        active = compiler.load<u8>(compiler.int_to_ptr(op_ptr + 16))
        active == 1 ? {
            compiler.store<u8>(compiler.int_to_ptr(op_ptr + 16), 0)
            return true
        }

        return false
    }
})

// ============================================================================
// Internal Methods
// ============================================================================

// Allocate operation ID and store pending callback
AsyncPool.alloc_pending_op = (self: MutPtr<AsyncPool>, callback: CompletionFn, user_data: u64) u64 {
    op_id = self.val.next_op_id
    self.val.next_op_id = self.val.next_op_id + 1

    slot = (op_id - 1) % MAX_PENDING_OPS as u64
    op_ptr = self.val.pending_ops + slot as i64 * 24
    compiler.store<u64>(compiler.int_to_ptr(op_ptr), user_data)
    compiler.store<i64>(compiler.int_to_ptr(op_ptr + 8), callback as i64)
    compiler.store<u8>(compiler.int_to_ptr(op_ptr + 16), 1)  // active = true

    return op_id
}

// Process a completion queue entry
AsyncPool.process_cqe = (self: MutPtr<AsyncPool>, cqe: CQE) void {
    op_id = cqe.user_data
    slot = (op_id - 1) % MAX_PENDING_OPS as u64
    op_ptr = self.val.pending_ops + slot as i64 * 24

    // Check if still active (not cancelled)
    active = compiler.load<u8>(compiler.int_to_ptr(op_ptr + 16))
    active == 1 ? {
        user_data = compiler.load<u64>(compiler.int_to_ptr(op_ptr))
        callback_ptr = compiler.load<i64>(compiler.int_to_ptr(op_ptr + 8))

        // Mark as complete
        compiler.store<u8>(compiler.int_to_ptr(op_ptr + 16), 0)
        self.val.pending_count = self.val.pending_count - 1

        // Invoke callback
        callback = callback_ptr as CompletionFn
        callback(user_data, cqe.res as i64)
    }
}

// ============================================================================
// Lifecycle
// ============================================================================

// Run the event loop until all pending operations complete
AsyncPool.run = (self: MutPtr<AsyncPool>) void {
    self.val.pending_count > 0 ? {
        self.val.wait()
    }
}

// Run the event loop for at most max_iterations
AsyncPool.run_for = (self: MutPtr<AsyncPool>, max_iterations: i32) i32 {
    iterations = 0
    iterations < max_iterations ? {
        self.val.pending_count == 0 ? { return iterations }
        self.val.wait()
        iterations = iterations + 1
    }
    return iterations
}

// Cleanup
AsyncPool.deinit = (self: MutPtr<AsyncPool>) void {
    // Close io_uring
    self.val.ring.mut_ref().close()

    // Free pending ops array
    ops_size = MAX_PENDING_OPS * 24
    compiler.raw_deallocate(compiler.int_to_ptr(self.val.pending_ops), ops_size)

    // Unmap memory pool
    compiler.syscall2(11, self.val.pool_base, self.val.pool_size)  // munmap
}

// ============================================================================
// Convenience: Check if allocator is async
// ============================================================================

AsyncPool.is_async = (self: AsyncPool) bool {
    return true
}
