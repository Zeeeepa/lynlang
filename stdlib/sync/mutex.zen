// Zen Standard Library: Mutex (Syscall-based)
// No FFI - uses compiler.syscall* intrinsics
// Fair mutex using futex for efficient blocking

{ compiler } = @std
{ futex_wait, futex_wake_one } = @std.sync.futex

// ============================================================================
// Mutex States
// ============================================================================
// 0 = unlocked
// 1 = locked, no waiters
// 2 = locked, with waiters

MUTEX_UNLOCKED = 0
MUTEX_LOCKED = 1
MUTEX_LOCKED_WAITERS = 2

// ============================================================================
// Mutex - Blocking mutual exclusion lock
// ============================================================================

Mutex: {
    state: i32
}

Mutex.new = () Mutex {
    return Mutex { state: MUTEX_UNLOCKED }
}

// Acquire the lock (blocks until acquired)
Mutex.lock = (self: MutPtr<Mutex>) void {
    // Fast path: try to acquire unlocked mutex
    old = compiler.atomic_cas(&self.val.state.ref() as Ptr<u64>, MUTEX_UNLOCKED, MUTEX_LOCKED)
    old == MUTEX_UNLOCKED ? { return }

    // Slow path: mutex is contended
    self.lock_slow()
}

// Slow path for lock acquisition
Mutex.lock_slow = (self: MutPtr<Mutex>) void {
    // Spin briefly before blocking
    spins = 0
    spins < 40 ? {
        state = compiler.atomic_load(&self.val.state.ref() as Ptr<u64>) as i32
        state == MUTEX_UNLOCKED ? {
            old = compiler.atomic_cas(&self.val.state.ref() as Ptr<u64>, MUTEX_UNLOCKED, MUTEX_LOCKED)
            old == MUTEX_UNLOCKED ? { return }
        }
        // Pause instruction would go here for better spin efficiency
        spins = spins + 1
    }

    // Mark as having waiters and block
    state = compiler.atomic_load(&self.val.state.ref() as Ptr<u64>) as i32
    state != MUTEX_LOCKED_WAITERS ? {
        old = compiler.atomic_cas(&self.val.state.ref() as Ptr<u64>, state, MUTEX_LOCKED_WAITERS)
    }

    // Block on futex until we acquire
    acquired = false
    acquired == false ? {
        futex_wait(&self.val.state.ref(), MUTEX_LOCKED_WAITERS)
        // Try to acquire, marking waiters since others may be waiting too
        old = compiler.atomic_cas(&self.val.state.ref() as Ptr<u64>, MUTEX_UNLOCKED, MUTEX_LOCKED_WAITERS)
        old == MUTEX_UNLOCKED ? { acquired = true }
    }
}

// Try to acquire without blocking
// Returns true if lock was acquired
Mutex.try_lock = (self: MutPtr<Mutex>) bool {
    old = compiler.atomic_cas(&self.val.state.ref() as Ptr<u64>, MUTEX_UNLOCKED, MUTEX_LOCKED)
    return old == MUTEX_UNLOCKED
}

// Release the lock
Mutex.unlock = (self: MutPtr<Mutex>) void {
    // Atomically set to unlocked and get previous state
    old = compiler.atomic_cas(&self.val.state.ref() as Ptr<u64>, MUTEX_LOCKED, MUTEX_UNLOCKED)

    // Fast path: no waiters
    old == MUTEX_LOCKED ? { return }

    // Had waiters - need to wake one
    old == MUTEX_LOCKED_WAITERS ? {
        compiler.atomic_store(&self.val.state.ref() as Ptr<u64>, MUTEX_UNLOCKED)
        futex_wake_one(&self.val.state.ref())
    }
}

// Check if mutex is locked (for debugging, not synchronization)
Mutex.is_locked = (self: Ptr<Mutex>) bool {
    state = compiler.atomic_load(&self.val.state.ref() as Ptr<u64>) as i32
    return state != MUTEX_UNLOCKED
}

// ============================================================================
// MutexGuard - RAII-style lock guard (manual scope management)
// ============================================================================

MutexGuard: {
    mutex: MutPtr<Mutex>
}

MutexGuard.new = (mutex: MutPtr<Mutex>) MutexGuard {
    mutex.lock()
    return MutexGuard { mutex: mutex }
}

MutexGuard.release = (self: MutPtr<MutexGuard>) void {
    self.val.mutex.unlock()
}
