// Zen Self-Hosting Compiler
// A compiler written in Zen that can compile Zen programs

// Module imports at top level (not in comptime!)
io = @std
fs = @std
string = @std
{ Vec, DynVec } = @std
core = @std
process = @std

// Compiler version
VERSION := "0.1.0"

// Token types for lexer
TokenType:
    // Literals
    .IntLiteral,
    FloatLiteral,
    StringLiteral,
    CharLiteral,
    BoolLiteral |
    
    // Identifiers and keywords
    .Identifier,
    Return,
    Loop,
    Break,
    Continue,
    Const,
    Enum,
    Struct,
    Comptime,
    Import |
    
    // Types
    .I32,
    I64,
    U32,
    U64,
    F32,
    F64,
    Bool,
    Void |
    
    // Operators
    .Plus,
    Minus,
    Star,
    Slash,
    Percent,
    Equal,
    NotEqual,
    Less,
    Greater,
    LessEqual,
    GreaterEqual,
    And,
    Or,
    Not,
    Assign,
    ColonAssign,
    PatternMatch |
    
    // Delimiters
    .LeftParen,
    RightParen,
    LeftBrace,
    RightBrace,
    LeftBracket,
    RightBracket,
    Comma,
    Dot,
    Semicolon,
    Colon,
    At |
    
    // Special
    .Eof |
    Error

// Token structure
Token: {
    type: TokenType,
    lexeme: string,
    line: i32,
    column: i32,
}

// Lexer state
Lexer: {
    source: string,
    current: i32,
    line: i32,
    column: i32,
    tokens: DynVec<Token>,
}

// Create a new lexer
new_lexer = (source: string) Lexer   {
    return Lexer {
        source: source,
        current: 0,
        line: 1,
        column: 1,
        tokens: DynVec.new(),
    }
}

// Check if at end of source
is_at_end = (lexer: Ptr<Lexer>) bool   {
    return lexer.current >= string.len(lexer.source)
}

// Get current character
peek = (lexer: Ptr<Lexer>) u8   {
    is_at_end(lexer) ?
        | true { return 0 }
        | false { }
    return string.char_at(lexer.source, lexer.current)
}

// Advance to next character
advance = (lexer: MutPtr<Lexer>) u8   {
    c := peek(lexer);
    lexer.current = lexer.current + 1;
    lexer.column = lexer.column + 1;
    c == '\n' ?
        | true {
            lexer.line = lexer.line + 1
            lexer.column = 1
        }
        | false { }
    return c
}

// Skip whitespace
skip_whitespace = (lexer: MutPtr<Lexer>) void   {
    loop {
        c := peek(lexer);
        c == ' ' || c == '\r' || c == '\t' || c == '\n' ?
            | true { advance(lexer) }
            | false {
                c == '/' && peek_next(lexer) == '/' ?
                    | true {
                        // Skip line comment
                        loop(() {
                            peek(lexer) == '\n' || is_at_end(lexer) ?
                                | true { break }
                                | false { }
                            advance(lexer)
                        })
                    }
                    | false { break }
            }
    }
}

// Peek next character
peek_next = (lexer: Ptr<Lexer>) u8   {
    lexer.current + 1 >= string.len(lexer.source) ?
        | true { return 0 }
        | false { }
    return string.char_at(lexer.source, lexer.current + 1)
}

// Add token to list
add_token = (lexer: MutPtr<Lexer>, type: TokenType, lexeme: string) void   {
    token := Token {
        type: type,
        lexeme: lexeme,
        line: lexer.line,
        column: lexer.column - string.len(lexeme),
    };
    DynVec.push(&lexer.tokens, token);
}

// Scan identifier or keyword
scan_identifier = (lexer: MutPtr<Lexer>) void   {
    start := lexer.current - 1;
    loop {
        c := peek(lexer);
        is_alpha(c) || is_digit(c) || c == '_' ?
            | true { advance(lexer) }
            | false { break }
    }
    
    text := string.slice(lexer.source, start, lexer.current);
    
    // Check for keywords
    type := text ?
        | "return" { TokenType.Return }
        | "loop" { TokenType.Loop }
        | "break" { TokenType.Break }
        | "continue" { TokenType.Continue }
        | "const" { TokenType.Const }
        | "enum" { TokenType.Enum }
        | "struct" { TokenType.Struct }
        | "comptime" { TokenType.Comptime }
        | "import" { TokenType.Import }
        | "i32" { TokenType.I32 }
        | "i64" { TokenType.I64 }
        | "u32" { TokenType.U32 }
        | "u64" { TokenType.U64 }
        | "f32" { TokenType.F32 }
        | "f64" { TokenType.F64 }
        | "bool" { TokenType.Bool }
        | "void" { TokenType.Void }
        | "true" | "false" { TokenType.BoolLiteral }
        | _ { TokenType.Identifier }
    
    add_token(lexer, type, text);
}

// Scan number literal
scan_number = (lexer: MutPtr<Lexer>) void   {
    start := lexer.current - 1;
    is_float := false;
    
    loop {
        c := peek(lexer);
        is_digit(c) ?
            | true { advance(lexer) }
            | false {
                c == '.' && !is_float && is_digit(peek_next(lexer)) ?
                    | true {
                        is_float = true
                        advance(lexer)
                    }
                    | false { break }
            }
    }
    
    text := string.slice(lexer.source, start, lexer.current);
    type := is_float ?
        | true { TokenType.FloatLiteral }
        | false { TokenType.IntLiteral }
    add_token(lexer, type, text);
}

// Scan string literal
scan_string = (lexer: MutPtr<Lexer>) void   {
    start := lexer.current;
    
    loop {
        peek(lexer) == '"' || is_at_end(lexer) ?
            | true { break }
            | false { }
        peek(lexer) == '\n' ?
            | true { lexer.line = lexer.line + 1 }
            | false { }
        advance(lexer);
    }
    
    is_at_end(lexer) ?
        | true {
            add_token(lexer, TokenType.Error, "Unterminated string")
            return
        }
        | false { }
    
    advance(lexer); // Closing "
    text := string.slice(lexer.source, start, lexer.current - 1);
    add_token(lexer, TokenType.StringLiteral, text);
}

// Check if character is alphabetic
is_alpha = (c: u8) bool   {
    return (c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')
}

// Check if character is digit
is_digit = (c: u8) bool   {
    return c >= '0' && c <= '9'
}

// Scan all tokens
scan_tokens = (lexer: MutPtr<Lexer>) void   {
    loop {
        is_at_end(lexer) ?
            | true { break }
            | false { }
        
        skip_whitespace(lexer)
        is_at_end(lexer) ?
            | true { break }
            | false { }
        
        c := advance(lexer);
        
        c ? {
            // Single character tokens
            | '(' { add_token(lexer, TokenType.LeftParen, "(") }
            | ')' { add_token(lexer, TokenType.RightParen, ")") }
            | '{' { add_token(lexer, TokenType.LeftBrace, "{") }
            | '}' { add_token(lexer, TokenType.RightBrace, "}") }
            | '[' { add_token(lexer, TokenType.LeftBracket, "[") }
            | ']' { add_token(lexer, TokenType.RightBracket, "]") }
            | ',' { add_token(lexer, TokenType.Comma, ",") }
            | '.' { add_token(lexer, TokenType.Dot, ".") }
            | ';' { add_token(lexer, TokenType.Semicolon, ";") }
            | '+' { add_token(lexer, TokenType.Plus, "+") }
            | '-' {
                peek(lexer) == '?' ?
                    | true {
                        advance(lexer)
                        add_token(lexer, TokenType.PatternMatch, "-?")
                    }
                    | false {
                        add_token(lexer, TokenType.Minus, "-")
                    }
            }
            | '*' { add_token(lexer, TokenType.Star, "*") }
            | '/' { add_token(lexer, TokenType.Slash, "/") }
            | '%' { add_token(lexer, TokenType.Percent, "%") }
            | '@' { add_token(lexer, TokenType.At, "@") }
            
            // Two character tokens
            | ':' {
                peek(lexer) == '=' ?
                    | true {
                        advance(lexer)
                        add_token(lexer, TokenType.ColonAssign, ":=")
                    }
                    | false {
                        add_token(lexer, TokenType.Colon, ":")
                    }
            }
            | '=' {
                peek(lexer) == '=' ?
                    | true {
                        advance(lexer)
                        add_token(lexer, TokenType.Equal, "==")
                    }
                    | false {
                        add_token(lexer, TokenType.Assign, "=")
                    }
            }
            | '!' {
                peek(lexer) == '=' ?
                    | true {
                        advance(lexer)
                        add_token(lexer, TokenType.NotEqual, "!=")
                    }
                    | false {
                        add_token(lexer, TokenType.Not, "!")
                    }
            }
            | '<' {
                peek(lexer) == '=' ?
                    | true {
                        advance(lexer)
                        add_token(lexer, TokenType.LessEqual, "<=")
                    }
                    | false {
                        add_token(lexer, TokenType.Less, "<")
                    }
            }
            | '>' {
                peek(lexer) == '=' ?
                    | true {
                        advance(lexer)
                        add_token(lexer, TokenType.GreaterEqual, ">=")
                    }
                    | false {
                        add_token(lexer, TokenType.Greater, ">")
                    }
            }
            | '&' {
                peek(lexer) == '&' ?
                    | true {
                        advance(lexer)
                        add_token(lexer, TokenType.And, "&&")
                    }
                    | false { }
            }
            | '|' {
                peek(lexer) == '|' ?
                    | true {
                        advance(lexer)
                        add_token(lexer, TokenType.Or, "||")
                    }
                    | false { }
            }
            
            // Literals
            | '"' { scan_string(lexer) }
            
            | _ {
                is_alpha(c) || c == '_' ?
                    | true { scan_identifier(lexer) }
                    | false {
                        is_digit(c) ?
                            | true { scan_number(lexer) }
                            | false { add_token(lexer, TokenType.Error, string.from_char(c)) }
                    }
            }
        }
    }
    
    add_token(lexer, TokenType.Eof, "");
}

// Simple AST nodes
AstNode:
    .Program { declarations: DynVec<Declaration> },
    FunctionDecl { name: string, params: DynVec<Parameter>, return_type: string, body: DynVec<Statement> },
    VariableDecl { name: string, type: string, value: Expression },
    ImportDecl { alias: string, path: string }

Declaration:
    .Function { name: string, params: DynVec<Parameter>, return_type: string, body: DynVec<Statement> },
    Variable { name: string, type: string, value: Expression },
    Import { alias: string, path: string }

Parameter: {
    name: string,
    type: string,
}

Statement:
    .Return { value: Expression },
    Expression { expr: Expression },
    PatternMatch { condition: Expression, then_branch: DynVec<Statement>, else_branch: DynVec<Statement> },
    Loop { body: DynVec<Statement> },
    Break,
    Continue

Expression:
    .IntLiteral { value: i32 },
    FloatLiteral { value: f64 },
    StringLiteral { value: string },
    BoolLiteral { value: bool },
    Identifier { name: string },
    Binary { left: Box<Expression>, op: string, right: Box<Expression> },
    Unary { op: string, operand: Box<Expression> },
    Call { callee: Box<Expression>, args: DynVec<Expression> },
    Member { object: Box<Expression>, member: string }

// Simple compiler driver
compile_file = (filename: string) bool   {
    // Read source file
    source_result := fs.read_file(filename);
    source_result.is_err() ?
        | true {
            io.print("Error reading file: ")
            io.println(filename)
            return false
        }
        | false { }
    
    source := source_result.unwrap();
    
    // Tokenize
    io.print("Lexing ");
    io.print(filename);
    io.print("... ");
    
    lexer := new_lexer(source);
    scan_tokens(&lexer);
    
    io.print("Found ");
    io.print_int(DynVec.len(lexer.tokens));
    io.println(" tokens");
    
    // For now, just verify we can tokenize
    DynVec.len(lexer.tokens) > 0 ?
        | true {
            io.println("✅ Lexing successful!")
            return true
        }
        | false {
            io.println("❌ Lexing failed!")
            return false
        }
}

// Main entry point
main = () i32   {
    io.print("Zen Self-Hosting Compiler v");
    io.println(VERSION);
    io.println("================================");
    
    // Test on a simple file
    test_file := "demos/simple_demo.zen";
    io.print("\nCompiling test file: ");
    io.println(test_file);
    
    compile_file(test_file) ?
        | true {
            io.println("\n✨ Compilation successful!")
            return 0
        }
        | false {
            io.println("\n❌ Compilation failed!")
            return 1
        }
}